#!/usr/bin/env python3
"""
Naver Blog Crawler Runner
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
import sys
import os
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawler import NaverBlogCrawler
from database.queries.brand_queries import BrandQueries
from config import Config


def crawl_single_brand(brand_official_name):
    """ë‹¨ì¼ ë¸Œëœë“œ í¬ë¡¤ë§ (ê³µì‹ëª… ê¸°ì¤€)"""
    brand = BrandQueries.get_brand_by_name(brand_official_name)
    if not brand:
        print(f"âŒ ë¸Œëœë“œëª… '{brand_official_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ” ë¸Œëœë“œ ì •ë³´:")
    print(f"   - ì´ë¦„: {brand['brand_official_name']}")

    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = NaverBlogCrawler()
    result = crawler.crawl_brand_blogs(
        brand_official_name=brand["brand_official_name"],
        max_pages=Config.MAX_PAGES_PER_BRAND,
        posts_per_page=Config.POSTS_PER_PAGE,
    )

    return result


def crawl_all_brands():
    """ëª¨ë“  ë¸Œëœë“œ í¬ë¡¤ë§"""
    brands = BrandQueries.list_brands()

    if not brands:
        print("âŒ í¬ë¡¤ë§í•  ë¸Œëœë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“‹ ì´ {len(brands)}ê°œ ë¸Œëœë“œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
    print("-" * 60)

    results = []
    for idx, brand in enumerate(brands, 1):
        print(f"\n[{idx}/{len(brands)}] ë¸Œëœë“œ: {brand['brand_official_name']}")
        try:
            result = crawl_single_brand(brand["brand_official_name"])
            if result:
                results.append(result)
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
    print("-" * 60)

    total_crawled = sum(r["crawled_count"] for r in results)
    total_saved = sum(r["saved_count"] for r in results)

    print(f"âœ… ì„±ê³µí•œ ë¸Œëœë“œ: {len(results)}ê°œ")
    print(f"ğŸ“„ í¬ë¡¤ë§í•œ í¬ìŠ¤íŠ¸: {total_crawled}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ëœ í¬ìŠ¤íŠ¸: {total_saved}ê°œ")


def main():
    parser = argparse.ArgumentParser(
        description="ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # íŠ¹ì • ë¸Œëœë“œ í¬ë¡¤ë§
  python run_crawler.py --brand-name 'uniformbridge'
  
  # ëª¨ë“  ë¸Œëœë“œ í¬ë¡¤ë§
  python run_crawler.py --all
  
  # ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ í¬ë¡¤ë§
  python run_crawler.py --brand-name 'kijun' --max-pages 20
        """,
    )

    # í¬ë¡¤ë§ ëŒ€ìƒ ì„ íƒ
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--brand-name", type=str, help="íŠ¹ì • ë¸Œëœë“œ ê³µì‹ëª…ìœ¼ë¡œ í¬ë¡¤ë§")
    group.add_argument("--all", action="store_true", help="ëª¨ë“  ë¸Œëœë“œ í¬ë¡¤ë§")

    # ì˜µì…˜
    parser.add_argument(
        "--max-pages",
        type=int,
        default=Config.MAX_PAGES_PER_BRAND,
        help=f"ë¸Œëœë“œë‹¹ ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: {Config.MAX_PAGES_PER_BRAND})",
    )

    args = parser.parse_args()

    # ì„¤ì • í™•ì¸
    Config.ensure_directories()

    print("ğŸš€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 60)

    try:
        if args.brand_name:
            # ì»¤ìŠ¤í…€ ì„¤ì • ì ìš©
            if args.max_pages:
                Config.MAX_PAGES_PER_BRAND = args.max_pages

            crawl_single_brand(args.brand_name)
        else:
            crawl_all_brands()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()

    print(f"\nâ° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()
