"""
E-E-A-T-GEO ë¶„ì„ê¸°
"""
import json
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from openai import OpenAI
from utils.file_utils import load_json_data, save_json_data
from utils.text_utils import parse_key_value_output


class EEATAnalyzer:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key)
    
    def analyze_posts(self, input_files):
        """ê²Œì‹œë¬¼ ë¶„ì„ ì‹¤í–‰"""
        processed_files = []
        
        for filename in input_files:
            if not self._file_exists(filename):
                print(f"âš ï¸ {filename} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
                
            try:
                print(f"\nğŸ”„ {filename} ì²˜ë¦¬ ì‹œì‘...")
                
                data = load_json_data(filename)
                if not data:
                    continue
                
                is_ugc_dataset = 'tagged' in filename.lower()
                print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {filename} ({len(data)}ê°œ ê²Œì‹œë¬¼)")
                print(f"ğŸ“‹ ë°ì´í„°ì…‹ ìœ í˜•: {'UGC í¬í•¨' if is_ugc_dataset else 'ê³µì‹ ê²Œì‹œë¬¼ ì „ìš©'}")
                
                detailed_analyses = []
                
                # ëª¨ë“  ê²Œì‹œë¬¼ ë¶„ì„
                for i, post in enumerate(data, 1):
                    print(f"ğŸ“ ê²Œì‹œë¬¼ {i}/{len(data)} ìƒì„¸ ë¶„ì„ ì¤‘...")
                    
                    analysis_result = self._analyze_single_post(post, i, is_ugc_dataset)
                    if analysis_result:
                        detailed_analyses.append(analysis_result)
                        print(f"  âœ… ì™„ë£Œ - ì ìˆ˜: {analysis_result['post_summary']['overall_score']:.1f}/100ì ")
                    else:
                        print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨")
                        continue
                
                # ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ë° ì €ì¥
                file_info = self._process_and_save_analysis(filename, detailed_analyses, is_ugc_dataset)
                if file_info:
                    processed_files.append(file_info)
                
            except Exception as e:
                print(f"âŒ {filename} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return processed_files[0]['content_file'] if processed_files else None
    
    def _file_exists(self, filename):
        """íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        import os
        return os.path.exists(filename)
    
    def _analyze_single_post(self, post, index, is_ugc):
        """ë‹¨ì¼ ê²Œì‹œë¬¼ ë¶„ì„"""
        url = post.get('href', '')
        content = post.get('content', '')
        date = post.get('date', '')
        comments = post.get('comments', [])
        
        print(f"  ğŸ“ URL: {url[:50]}...")
        print(f"  ğŸ·ï¸ íƒ€ì…: {'UGC' if is_ugc else 'ê³µì‹ ê²Œì‹œë¬¼'}")
        
        if is_ugc:
            analysis_prompt = self._get_ugc_analysis_prompt(url, content, date, len(comments))
        else:
            analysis_prompt = self._get_official_analysis_prompt(url, content, date, len(comments))
        
        try:
            print(f"  ğŸ¤– AI ìƒì„¸ ë¶„ì„ ì¤‘...")
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": analysis_prompt}],
                temperature=0.3
            )
            
            analysis_text = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ
            start_idx = analysis_text.find('{')
            end_idx = analysis_text.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                analysis_json = json.loads(analysis_text[start_idx:end_idx])
            else:
                raise ValueError("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ìƒì„±
            return self._structure_analysis_result(post, index, analysis_json, is_ugc, url, date, content, comments)
            
        except Exception as e:
            print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_ugc_analysis_prompt(self, url, content, date, comment_count):
        """UGC ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        return f"""
        You are a 'Senior Marketing Strategist' analyzing a KIJUN brand UGC Instagram post.
        Perform a comprehensive 3-step analysis and provide final scoring.
        
        POST DATA:
        URL: {url}
        Caption: {content}
        Date: {date}
        Comments: {comment_count}
        
        STEP 1 - Product & TPO Analysis:
        From the image and caption, identify the KIJUN product and analyze the TPO (Time, Place, Occasion).
        Score how well the product fits the TPO situation (0-100 points, where 100 is perfect fit).
        
        STEP 2 - Styling Analysis:
        Define the styling mood, list paired items, and score the styling creativity (0-100 points).
        Focus on how creatively the KIJUN item is styled with other pieces.
        
        STEP 3 - Sentiment Analysis:
        Analyze the user's emotional expression and satisfaction level (0-100 points).
        Look for positive emotions, satisfaction indicators, and brand affinity.
        
        Provide your analysis in JSON format with detailed Korean explanations:
        {{
            "product_guess": "ì¶”ì • ì œí’ˆëª…",
            "tpo_analysis": "TPO ìƒí™© ë¶„ì„",
            "tpo_score": ì ìˆ˜ (0-100),
            "tpo_reason": "TPO ì ì ˆì„± í‰ê°€ ì´ìœ ",
            "styling_mood": "ìŠ¤íƒ€ì¼ë§ ë¬´ë“œ ì •ì˜",
            "paired_items": "í•¨ê»˜ ë§¤ì¹˜ëœ ì•„ì´í…œë“¤",
            "styling_score": ì ìˆ˜ (0-100),
            "styling_reason": "ìŠ¤íƒ€ì¼ë§ ì°½ì˜ì„± í‰ê°€ ì´ìœ ",
            "sentiment_score": ì ìˆ˜ (0-100),
            "sentiment_reason": "ê°ì •ì  ì–´í•„ ë° ë§Œì¡±ë„ ë¶„ì„",
            "brand_relevance": ì ìˆ˜ (0-100),
            "brand_reason": "ë¸Œëœë“œ ì—°ê´€ì„± ë° ì¶©ì„±ë„ ë¶„ì„",
            "visual_appeal": ì ìˆ˜ (0-100),
            "visual_reason": "ì‹œê°ì  ë§¤ë ¥ë„ í‰ê°€",
            "synergy_score": ì ìˆ˜ (0-100),
            "synergy_reason": "ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì˜ ì—°ê²°ì„± ë¶„ì„",
            "overall_score": í‰ê· ì ìˆ˜ (0-100),
            "summary": "UGC ì „ì²´ ìš”ì•½ ë° ë¸Œëœë“œ ê°€ì¹˜",
            "overall_suggestion": "UGC í™œìš© ë° ë¸Œëœë“œ ë§ˆì¼€íŒ… ì œì•ˆ"
        }}
        """
    
    def _get_official_analysis_prompt(self, url, content, date, comment_count):
        """ê³µì‹ ê²Œì‹œë¬¼ ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        return f"""
        You are a content quality inspector for the fashion brand 'KIJUN', specializing in E-E-A-T and Generative Engine Optimization (GEO).
        Analyze this official KIJUN Instagram post comprehensively.
        
        POST DATA:
        URL: {url}
        Caption: {content}
        Date: {date}
        Comments: {comment_count}
        
        [KIJUN E-E-A-T-GEO Analysis Framework]
        
        **SCORING INSTRUCTIONS: Use 0-100 point scale for ALL scores where:**
        - 90-100: Exceptional quality, industry-leading standard
        - 80-89: Very good quality, above average performance  
        - 70-79: Good quality, meets standard expectations
        - 60-69: Fair quality, some improvement needed
        - 50-59: Below average, significant improvement required
        - 0-49: Poor quality, major overhaul needed
        
        1. **Experience (0-100 points)**: Does this content show real user experience or authentic brand experience?
        2. **Expertise (0-100 points)**: Does this content demonstrate professional fashion knowledge and brand expertise?
        3. **Authoritativeness (0-100 points)**: Does this content align with KIJUN's brand authority and aesthetic consistency?
        4. **Trustworthiness (0-100 points)**: Is this content authentic, reliable, and free from exaggeration?
        
        5. **GEO (Generative Engine Optimization) - Detailed Breakdown (Each 0-100 points)**:
           - **Clarity & Specificity**: How clear and specific are product names, keywords, and information?
           - **Structured Information**: Is information well-structured for AI parsing (bullet points, clear hierarchy)?
           - **Contextual Richness**: Does it provide rich context (collection info, styling tips, season relevance)?
           - **Visual-Text Alignment**: Do images and text strongly support each other?
           - **Timeliness & Event-Relevance**: Is content relevant to current time, season, or events?
           - **Originality**: Is the creative expression fresh, unique, and avoiding clichÃ©s?
        
        6. **Content Synergy Analysis (Each 0-100 points)**:
           - **Visual Coherence**: Consistency in mood, color palette, and style across all images
           - **Narrative Flow**: Logical sequence and storytelling flow of images
           - **Alignment with Caption**: How well caption unifies and enhances all visuals
        
        Provide comprehensive analysis in JSON format with Korean explanations:
        {{
            "experience_score": ì ìˆ˜ (0-100),
            "experience_reason": "ì‹¤ì œ ê²½í—˜ ê¸°ë°˜ ì½˜í…ì¸  ì—¬ë¶€ì™€ ì‚¬ìš©ì ê´€ì  ë°˜ì˜ë„ ë¶„ì„",
            "expertise_score": ì ìˆ˜ (0-100),
            "expertise_reason": "íŒ¨ì…˜ ì „ë¬¸ì„±ê³¼ ë¸Œëœë“œ ë…¸í•˜ìš° ì „ë‹¬ë ¥ ë¶„ì„",
            "authoritativeness_score": ì ìˆ˜ (0-100),
            "authoritativeness_reason": "ë¸Œëœë“œ ê¶Œìœ„ì„±ê³¼ ì¼ê´€ëœ ë¯¸ì  ê¸°ì¤€ ìœ ì§€ë„ ë¶„ì„",
            "trustworthiness_score": ì ìˆ˜ (0-100),
            "trustworthiness_reason": "ì‹ ë¢°ì„±ê³¼ ì •ë³´ì˜ ì •í™•ì„±, ê³¼ì¥ ì—†ëŠ” í‘œí˜„ ë¶„ì„",
            "geo_clarity_score": ì ìˆ˜ (0-100),
            "geo_clarity_reason": "ì œí’ˆëª…, í‚¤ì›Œë“œì˜ ëª…í™•ì„±ê³¼ AI ì´í•´ë„ ë¶„ì„",
            "geo_structure_score": ì ìˆ˜ (0-100),
            "geo_structure_reason": "ì •ë³´ êµ¬ì¡°í™”ì™€ AI íŒŒì‹± ìš©ì´ì„± ë¶„ì„",
            "geo_context_score": ì ìˆ˜ (0-100),
            "geo_context_reason": "ì»¨í…ìŠ¤íŠ¸ í’ë¶€ì„±ê³¼ í™œìš© ë§¥ë½ ì œê³µë„ ë¶„ì„",
            "geo_alignment_score": ì ìˆ˜ (0-100),
            "geo_alignment_reason": "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •í•©ì„±ê³¼ ìƒí˜¸ ë³´ì™„ì„± ë¶„ì„",
            "geo_timeliness_score": ì ìˆ˜ (0-100),
            "geo_timeliness_reason": "ì‹œì˜ì„±ê³¼ ê³„ì ˆê°, ì´ë²¤íŠ¸ ì—°ê´€ì„± ë¶„ì„",
            "geo_originality_score": ì ìˆ˜ (0-100),
            "geo_originality_reason": "ì°½ì˜ì  í‘œí˜„ê³¼ ë…ì°½ì„±, ì°¨ë³„í™” ìš”ì†Œ ë¶„ì„",
            "visual_coherence_score": ì ìˆ˜ (0-100),
            "visual_coherence_reason": "ì´ë¯¸ì§€ ê°„ ì‹œê°ì  ì¼ê´€ì„±ê³¼ í†µì¼ê° ë¶„ì„",
            "narrative_flow_score": ì ìˆ˜ (0-100),
            "narrative_flow_reason": "ì„œì‚¬ì  íë¦„ê³¼ ì´ë¯¸ì§€ ìˆœì„œì˜ ë…¼ë¦¬ì„± ë¶„ì„",
            "alignment_with_caption_score": ì ìˆ˜ (0-100),
            "alignment_with_caption_reason": "ìº¡ì…˜ê³¼ ì´ë¯¸ì§€ì˜ ì—°ê²°ì„±ê³¼ ë©”ì‹œì§€ í†µí•©ë„ ë¶„ì„",
            "overall_score": ì „ì²´ í‰ê· ì ìˆ˜ (0-100),
            "category_averages": {{
                "eeat_avg": "E-E-A-T 4ê°œ í•­ëª© í‰ê· ",
                "geo_avg": "GEO 6ê°œ í•­ëª© í‰ê· ", 
                "synergy_avg": "ì‹œë„ˆì§€ 3ê°œ í•­ëª© í‰ê· "
            }},
            "summary": "ì „ì²´ ë¶„ì„ ìš”ì•½ê³¼ ë¸Œëœë“œ ì½˜í…ì¸ ë¡œì„œì˜ ê°€ì¹˜ í‰ê°€",
            "overall_suggestion": "êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì œì•ˆì‚¬í•­"
        }}
        """
    
    def _structure_analysis_result(self, post, index, analysis_json, is_ugc, url, date, content, comments):
        """ë¶„ì„ ê²°ê³¼ êµ¬ì¡°í™”"""
        post_analysis = {
            "post_url": url,
            "original_post_data": {
                "number": index,
                "href": url,
                "date": date,
                "content": content,
                "img": post.get('img', []),
                "comments": comments
            },
            "post_summary": {
                "summary": analysis_json.get('summary', ''),
                "overall_score": analysis_json.get('overall_score', 0),
                "category_averages": analysis_json.get('category_averages', {}),
                "overall_suggestion": analysis_json.get('overall_suggestion', '')
            }
        }
        
        # UGCì™€ ê³µì‹ ê²Œì‹œë¬¼ì— ë”°ë¥¸ ë‹¤ë¥¸ êµ¬ì¡°
        if is_ugc:
            post_analysis["ugc_analysis"] = {
                "product_guess": analysis_json.get('product_guess', ''),
                "tpo_analysis": analysis_json.get('tpo_analysis', ''),
                "tpo_score": str(analysis_json.get('tpo_score', 0)),
                "tpo_reason": analysis_json.get('tpo_reason', ''),
                "styling_mood": analysis_json.get('styling_mood', ''),
                "paired_items": analysis_json.get('paired_items', ''),
                "styling_score": str(analysis_json.get('styling_score', 0)),
                "styling_reason": analysis_json.get('styling_reason', ''),
                "sentiment_score": str(analysis_json.get('sentiment_score', 0)),
                "sentiment_reason": analysis_json.get('sentiment_reason', ''),
                "brand_relevance": str(analysis_json.get('brand_relevance', 0)),
                "brand_reason": analysis_json.get('brand_reason', ''),
                "visual_appeal": str(analysis_json.get('visual_appeal', 0)),
                "visual_reason": analysis_json.get('visual_reason', ''),
                "synergy_score": str(analysis_json.get('synergy_score', 0)),
                "synergy_reason": analysis_json.get('synergy_reason', '')
            }
        else:
            post_analysis["synergy_analysis"] = {
                "visual_coherence_score": str(analysis_json.get('visual_coherence_score', 0)),
                "visual_coherence_reason": analysis_json.get('visual_coherence_reason', ''),
                "narrative_flow_score": str(analysis_json.get('narrative_flow_score', 0)),
                "narrative_flow_reason": analysis_json.get('narrative_flow_reason', ''),
                "alignment_with_caption_score": str(analysis_json.get('alignment_with_caption_score', 0)),
                "alignment_with_caption_reason": analysis_json.get('alignment_with_caption_reason', '')
            }
            
            post_analysis["eeat_geo_analysis"] = {
                "experience_score": str(analysis_json.get('experience_score', 0)),
                "experience_reason": analysis_json.get('experience_reason', ''),
                "expertise_score": str(analysis_json.get('expertise_score', 0)),
                "expertise_reason": analysis_json.get('expertise_reason', ''),
                "authoritativeness_score": str(analysis_json.get('authoritativeness_score', 0)),
                "authoritativeness_reason": analysis_json.get('authoritativeness_reason', ''),
                "trustworthiness_score": str(analysis_json.get('trustworthiness_score', 0)),
                "trustworthiness_reason": analysis_json.get('trustworthiness_reason', ''),
                "geo_clarity_score": str(analysis_json.get('geo_clarity_score', 0)),
                "geo_clarity_reason": analysis_json.get('geo_clarity_reason', ''),
                "geo_structure_score": str(analysis_json.get('geo_structure_score', 0)),
                "geo_structure_reason": analysis_json.get('geo_structure_reason', ''),
                "geo_context_score": str(analysis_json.get('geo_context_score', 0)),
                "geo_context_reason": analysis_json.get('geo_context_reason', ''),
                "geo_alignment_score": str(analysis_json.get('geo_alignment_score', 0)),
                "geo_alignment_reason": analysis_json.get('geo_alignment_reason', ''),
                "geo_timeliness_score": str(analysis_json.get('geo_timeliness_score', 0)),
                "geo_timeliness_reason": analysis_json.get('geo_timeliness_reason', ''),
                "geo_originality_score": str(analysis_json.get('geo_originality_score', 0)),
                "geo_originality_reason": analysis_json.get('geo_originality_reason', '')
            }
        
        post_analysis["individual_analyses"] = [{
            "overall_suggestion": analysis_json.get('overall_suggestion', '')
        }]
        
        return post_analysis
    
    def _process_and_save_analysis(self, filename, detailed_analyses, is_ugc_dataset):
        """ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ë° ì €ì¥"""
        # ì ìˆ˜ë³„ ì •ë ¬ ë° ì½˜í…ì¸  ìƒì„±
        sorted_analyses = sorted(detailed_analyses, key=lambda x: x['post_summary']['overall_score'])
        
        # í•˜ìœ„/ìƒìœ„ ì„ íƒ
        low_score_posts = sorted_analyses[:2] if len(sorted_analyses) >= 2 else sorted_analyses[:1]
        high_score_posts = sorted_analyses[-2:] if len(sorted_analyses) >= 2 else sorted_analyses[-1:]
        
        print(f"\nğŸ¯ {filename} ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(detailed_analyses)}ê°œ ê²Œì‹œë¬¼ ë¶„ì„")
        
        # ê°œì„ ëœ ì½˜í…ì¸  ìƒì„±
        print(f"\nğŸ’¡ {filename} ê°œì„ ëœ ì½˜í…ì¸  ì œì•ˆ ìƒì„± ì¤‘...")
        
        low_score_revisions = self._generate_content_revisions(low_score_posts)
        high_score_analyses = self._generate_success_analyses(high_score_posts)
        
        # UGC í¬í•¨ ì—¬ë¶€ í™•ì¸
        ugc_count = len([p for p in detailed_analyses if 'ugc_analysis' in p])
        has_ugc = ugc_count > 0
        
        print(f"\nğŸ“Š {filename} ë¶„ì„ ê²°ê³¼ êµ¬ì„±:")
        print(f"  - ì „ì²´ ê²Œì‹œë¬¼: {len(detailed_analyses)}ê°œ")
        print(f"  - UGC ê²Œì‹œë¬¼: {ugc_count}ê°œ")
        print(f"  - ê³µì‹ ê²Œì‹œë¬¼: {len(detailed_analyses) - ugc_count}ê°œ")
        
        # íŒŒì¼ëª… ê²°ì •
        base_name = filename.replace('.json', '')
        if has_ugc:
            eeatg_filename = os.path.join('output', f'{base_name}_ugc_combined_analysis.json')
            content_filename = os.path.join('output', f'{base_name}_ugc_generated_content.json')
            print(f"ğŸ’¡ UGC í¬í•¨ ë¶„ì„ìœ¼ë¡œ íŒŒì¼ëª…: {eeatg_filename}")
        else:
            eeatg_filename = os.path.join('output', f'{base_name}_eeatg_analysis.json')
            content_filename = os.path.join('output', f'{base_name}_generated_content.json')
            print(f"ğŸ’¡ ê³µì‹ ê²Œì‹œë¬¼ ì „ìš© ë¶„ì„ íŒŒì¼ëª…: {eeatg_filename}")
        
        # íŒŒì¼ ì €ì¥
        save_json_data(detailed_analyses, eeatg_filename)
        
        content_suggestions = {
            "low_score_revisions": low_score_revisions,
            "high_score_analyses": high_score_analyses
        }
        save_json_data(content_suggestions, content_filename)
        
        print(f"âœ… {filename} ì²˜ë¦¬ ì™„ë£Œ - íŒŒì¼ ì €ì¥: {eeatg_filename}, {content_filename}")
        
        return {
            'input_file': filename,
            'analysis_file': eeatg_filename,
            'content_file': content_filename,
            'is_ugc': is_ugc_dataset,
            'post_count': len(detailed_analyses)
        }
    
    def _generate_content_revisions(self, low_score_posts):
        """í•˜ìœ„ ì ìˆ˜ ê²Œì‹œë¬¼ ê°œì„ ì•ˆ ìƒì„±"""
        revisions = []
        
        for post in low_score_posts:
            print(f"  ğŸ“ í•˜ìœ„ ì ìˆ˜ ê²Œì‹œë¬¼ ê°œì„ ì•ˆ ìƒì„± ì¤‘...")
            
            revision_prompt = f"""
            You are a senior Creative Director for the fashion brand 'KIJUN'.
            An underperforming post needs revision based on comprehensive analysis.
            
            ORIGINAL POST DATA:
            URL: {post['post_url']}
            Original Caption: {post['original_post_data']['content']}
            Current Score: {post['post_summary']['overall_score']}
            Analysis Summary: {post['post_summary']['summary']}
            Improvement Suggestions: {post['post_summary']['overall_suggestion']}
            
            POST TYPE: {'UGC' if 'ugc_analysis' in post else 'Official Brand Post'}
            
            TASK: Based on the analysis findings, create improved content that addresses the low-scoring areas.
            Focus on improving E-E-A-T-GEO factors (for official posts) or UGC engagement factors (for UGC posts).
            
            **CRITICAL INSTRUCTION: Your entire response MUST be in KOREAN and in Key-Value format with '|||' as separator.**
            
            Key-Value Format Example:
            new_caption_v1: (ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜ì˜í•œ ê°œì„ ëœ ìº¡ì…˜ ë²„ì „ 1)
            |||
            new_caption_v2: (ë‹¤ë¥¸ ì ‘ê·¼ë²•ì˜ ê°œì„ ëœ ìº¡ì…˜ ë²„ì „ 2)
            |||
            hashtags: #í‚¤ì¤€ #ë””ìì´ë„ˆë¸Œëœë“œ #ì‹ ìƒí’ˆ (ìƒˆë¡œìš´ ì»¨ì…‰ì— ìµœì í™”ëœ í•´ì‹œíƒœê·¸)
            |||
            new_image_idea: (ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œì•ˆí•˜ëŠ” ìƒˆë¡œìš´ ì´ë¯¸ì§€ ì»¨ì…‰ ë° êµ¬ì„± ì•„ì´ë””ì–´)
            |||
            original_post_url: {post['post_url']}
            """
            
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": revision_prompt}],
                    temperature=0.5
                )
                
                revision_text = response.choices[0].message.content
                revision_json = parse_key_value_output(revision_text)
                if revision_json:
                    revisions.append(revision_json)
                
            except Exception as e:
                print(f"    âŒ ê°œì„ ì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return revisions
    
    def _generate_success_analyses(self, high_score_posts):
        """ìƒìœ„ ì ìˆ˜ ê²Œì‹œë¬¼ ì„±ê³µ ìš”ì¸ ë¶„ì„"""
        analyses = []
        
        for post in high_score_posts:
            print(f"  ğŸ† ìƒìœ„ ì ìˆ˜ ê²Œì‹œë¬¼ ì„±ê³µ ìš”ì¸ ë¶„ì„ ì¤‘...")
            
            success_prompt = f"""
            You are a 'Lead Content Strategist' analyzing a high-performing KIJUN Instagram post.
            Identify the key success factors based on comprehensive performance analysis.
            
            HIGH-PERFORMING POST DATA:
            URL: {post['post_url']}
            Caption: {post['original_post_data']['content']}
            Excellent Score: {post['post_summary']['overall_score']}
            Analysis Summary: {post['post_summary']['summary']}
            
            POST TYPE: {'UGC' if 'ugc_analysis' in post else 'Official Brand Post'}
            
            TASK: Analyze WHY this post was highly effective and extract actionable insights.
            Focus on the strongest performing elements that can be replicated in future content.
            
            **CRITICAL INSTRUCTION: Your entire response MUST be in KOREAN and in Key-Value format with '|||' as separator.**
            
            Key-Value Format Example:
            success_factor_1: (ê°€ì¥ í•µì‹¬ì ì¸ ì„±ê³µ ìš”ì¸ 1)
            |||
            success_factor_2: (ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ì„±ê³µ ìš”ì¸ 2)
            |||
            key_takeaway: (ì´ ê²Œì‹œë¬¼ë¡œë¶€í„° ì–»ì„ ìˆ˜ ìˆëŠ” ê°€ì¥ ì¤‘ìš”í•œ êµí›ˆê³¼ ë‹¤ìŒ ì•¡ì…˜ ì•„ì´í…œ)
            |||
            original_post_url: {post['post_url']}
            """
            
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": success_prompt}],
                    temperature=0.5
                )
                
                success_text = response.choices[0].message.content
                success_json = parse_key_value_output(success_text)
                if success_json:
                    analyses.append(success_json)
                
            except Exception as e:
                print(f"    âŒ ì„±ê³µ ìš”ì¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return analyses
