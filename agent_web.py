#!/usr/bin/env python3
"""
ë‹¤ì¤‘ í˜ì´ì§€ ì „ìš© LangGraph SEO/GEO(Generative Engine Optimization) ìµœì í™” ì‹œìŠ¤í…œ
@tool ë°ì½”ë ˆì´í„° + LangGraph ì›Œí¬í”Œë¡œìš° + ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
"""

from typing import Dict, List, Optional, TypedDict, Literal, Annotated, Set, Any
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from dataclasses import dataclass, asdict
import asyncio
import os
import json
import time
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse, urljoin
from datetime import datetime
import mysql.connector
from mysql.connector import Error
from dotenv import load_dotenv
import os

# from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Selenium ì„í¬íŠ¸ (ì„ íƒì )
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import WebDriverException

    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

    # Options í´ë˜ìŠ¤ë¥¼ ë”ë¯¸ë¡œ ì •ì˜
    class Options:
        def __init__(self):
            pass

        def add_argument(self, arg):
            pass

    print("âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ HTTP í¬ë¡¤ë§ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")


class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì¿¼ë¦¬ ê´€ë¦¬"""

    def __init__(self):
        self.connection = None
        self.connect()

    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.connection = mysql.connector.connect(
                host=os.getenv("DB_HOST", "localhost"),
                port=int(os.getenv("DB_PORT", "3306")),
                database=os.getenv("DB_NAME", "modular_agents_db"),
                user=os.getenv("DB_USER", "root"),
                password=os.getenv("DB_PASSWORD", "1234"),
            )
            print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        except Error as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    def get_latest_brand_url(self):
        """ê°€ì¥ ìµœê·¼ì— ì—…ë°ì´íŠ¸ëœ ë¸Œëœë“œì˜ official_site_url ê°€ì ¸ì˜¤ê¸°"""
        try:
            cursor = self.connection.cursor(dictionary=True)
            query = """
            SELECT brand_official_name, official_site_url, updated_at
            FROM 01_brands 
            WHERE official_site_url IS NOT NULL 
            AND official_site_url != ''
            ORDER BY updated_at DESC 
            LIMIT 1
            """
            cursor.execute(query)
            result = cursor.fetchone()
            cursor.close()

            if result:
                print(
                    f"ğŸ“Š ìµœì‹  ë¸Œëœë“œ: {result['brand_official_name']} - {result['official_site_url']}"
                )
                return result
            else:
                print("âŒ ë¸Œëœë“œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

        except Error as e:
            print(f"âŒ ë¸Œëœë“œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.connection and self.connection.is_connected():
            self.connection.close()


# ===== LangGraph State ì •ì˜ =====
class MultiPageSEOWorkflowState(TypedDict):
    """ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""

    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
    messages: Annotated[List, add_messages]

    # ì‚¬ìš©ì ì…ë ¥
    user_url: str
    user_mode: Literal["analyze", "optimize", "full"]
    api_key: str
    max_pages: int

    # ì›Œí¬í”Œë¡œìš° ì§„í–‰ ìƒíƒœ
    current_stage: str
    next_action: str

    # í¬ë¡¤ë§ ê²°ê³¼
    crawl_results: List[Dict]
    crawled_files: List[str]

    # í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼
    page_analyses: List[Dict]

    # ì¢…í•© ë¶„ì„ ê²°ê³¼
    site_seo_analysis: Dict
    site_geo_analysis: Dict
    site_performance: Dict
    site_structured_data: Dict
    site_keywords: Dict

    # ìµœì í™” ê²°ê³¼
    optimized_pages: List[Dict]
    optimization_summary: Dict

    meta_results: Dict  # ë©”íƒ€íƒœê·¸ ìƒì„± ê²°ê³¼
    jsonld_results: Dict  # JSON-LD ìƒì„± ê²°ê³¼
    faq_results: Dict  # FAQ ìƒì„± ê²°ê³¼
    final_optimization: List[Dict]  # ìµœì¢… HTML íŒŒì¼ ì •ë³´
    final_html_files: List[Dict]

    # ì„¤ì •
    business_type: str
    target_keywords: str

    # ê²°ê³¼
    output_files: List[str]
    final_summary: Dict


# ===== JSON-LD í…œí”Œë¦¿ ì‹œìŠ¤í…œ =====
BUSINESS_SCHEMA_TEMPLATES = {
    "ë²•ë¬´ë²•ì¸": {
        "main": {
            "@context": "https://schema.org",
            "@type": ["LegalService", "ProfessionalService"],
            "name": "{company_name}",
            "description": "{description}",
            "serviceType": "ë²•ë¥  ì„œë¹„ìŠ¤",
            "areaServed": "ëŒ€í•œë¯¼êµ­",
            "url": "{url}",
        },
        "about": {
            "@context": "https://schema.org",
            "@type": "AboutPage",
            "mainEntity": {"@type": "LegalService", "name": "{company_name}"},
        },
        "service": {
            "@context": "https://schema.org",
            "@type": "Service",
            "serviceType": "ë²•ë¥  ìƒë‹´",
            "provider": {"@type": "LegalService", "name": "{company_name}"},
        },
    },
    "ë³‘ì›": {
        "main": {
            "@context": "https://schema.org",
            "@type": "MedicalOrganization",
            "name": "{company_name}",
            "description": "{description}",
            "medicalSpecialty": "ì¢…í•©ì˜ë£Œ",
            "url": "{url}",
        }
    },
    "ì‡¼í•‘ëª°": {
        "main": {
            "@context": "https://schema.org",
            "@type": "OnlineStore",
            "name": "{company_name}",
            "description": "{description}",
            "url": "{url}",
            "paymentAccepted": ["ì‹ ìš©ì¹´ë“œ", "ê³„ì¢Œì´ì²´", "ë¬´í†µì¥ì…ê¸ˆ"],
        },
        "product": {
            "@context": "https://schema.org",
            "@type": "Product",
            "name": "{product_name}",
            "description": "{product_description}",
            "availability": "InStock",
        },
    },
    "ITíšŒì‚¬": {
        "main": {
            "@context": "https://schema.org",
            "@type": "Corporation",
            "name": "{company_name}",
            "description": "{description}",
            "industry": "ì •ë³´ê¸°ìˆ ",
            "url": "{url}",
        }
    },
    "ì¹´í˜": {
        "main": {
            "@context": "https://schema.org",
            "@type": "CafeOrCoffeeShop",
            "name": "{company_name}",
            "description": "{description}",
            "cuisine": "ì¹´í˜, ìŒë£Œ",
            "url": "{url}",
        }
    },
    # ê¸°ë³¸ í…œí”Œë¦¿
    "default": {
        "main": {
            "@context": "https://schema.org",
            "@type": "Organization",
            "name": "{company_name}",
            "description": "{description}",
            "url": "{url}",
        },
        "about": {
            "@context": "https://schema.org",
            "@type": "AboutPage",
            "mainEntity": {"@type": "Organization", "name": "{company_name}"},
        },
        "contact": {
            "@context": "https://schema.org",
            "@type": "ContactPage",
            "mainEntity": {"@type": "Organization", "name": "{company_name}"},
        },
        "faq": {
            "@context": "https://schema.org",
            "@type": "AboutPage",
            "mainEntity": {"@type": "Organization", "name": "{company_name}"},
        },
        "team": {
            "@context": "https://schema.org",
            "@type": "AboutPage",
            "mainEntity": {"@type": "Organization", "name": "{company_name}"},
        },
        "other": {
            "@context": "https://schema.org",
            "@type": "Organization",
            "name": "{company_name}",
            "description": "{description}",
            "url": "{url}",
        },
        "product": {
            "@context": "https://schema.org",
            "@type": "Product",
            "name": "{company_name}",
            "description": "{description}",
        },
    },
}


class StreamlinedSEOGEOCrawler:
    """LLM ë¶„ì„ì„ ìœ„í•œ ë‹¤ì¤‘ í˜ì´ì§€ HTML ìˆ˜ì§‘ í¬ë¡¤ëŸ¬"""

    def __init__(self, base_url: str, max_pages: int = 9):
        self.base_url = base_url.rstrip("/")
        self.max_pages = max_pages
        self.max_product_pages = 3
        self.parsed_base = urlparse(base_url)
        self.crawled_urls: Set[str] = set()
        self.results: List[Dict[str, Any]] = []

        # Chrome ì˜µì…˜ ì„¤ì • (Selenium ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
        if SELENIUM_AVAILABLE:
            self.chrome_options = Options()
            self.chrome_options.add_argument("--headless")
            self.chrome_options.add_argument("--no-sandbox")
            self.chrome_options.add_argument("--disable-dev-shm-usage")
            self.chrome_options.add_argument("--disable-gpu")
            self.chrome_options.add_argument("--window-size=1920,1080")
            self.chrome_options.add_argument(
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
        else:
            self.chrome_options = None

        # ìš°ì„ ìˆœìœ„ í˜ì´ì§€ ì •ì˜
        self.priority_pages = [
            ("main", ["", "/", "/index", "/index.html", "/home", "/main"]),
            (
                "about",
                [
                    "/about",
                    "/info",
                    "/company",
                    "/ì†Œê°œ",
                    "/íšŒì‚¬ì†Œê°œ",
                    "/íšŒì‚¬",
                    "/ê¸°ì—…ì†Œê°œ",
                    "/company-info",
                    "/about-us",
                ],
            ),
            (
                "service",
                [
                    "/service",
                    "/services",
                    "/ì„œë¹„ìŠ¤",
                    "/ì—…ë¬´ë¶„ì•¼",
                    "/ì „ë¬¸ë¶„ì•¼",
                    "/ì—…ë¬´ì˜ì—­",
                    "/practice-areas",
                ],
            ),
            (
                "product",
                [
                    "/product",
                    "/products",
                    "/goods",
                    "/shop",
                    "/ìƒí’ˆ",
                    "/ì œí’ˆ",
                    "/item",
                    "/ì•„ì´í…œ",
                ],
            ),
            (
                "faq",
                [
                    "/faq",
                    "/FAQ",
                    "/ì§ˆë¬¸",
                    "/ë¬¸ì˜ì‚¬í•­",
                    "/cs/faq",
                    "/help/faq",
                    "/ìì£¼ë¬»ëŠ”ì§ˆë¬¸",
                    "/ê³ ê°ì§€ì›",
                    "/help",
                ],
            ),
            (
                "contact",
                [
                    "/contact",
                    "/contacts",
                    "/ì—°ë½ì²˜",
                    "/ë¬¸ì˜",
                    "/ìƒë‹´ì‹ ì²­",
                    "/ì˜¤ì‹œëŠ”ê¸¸",
                    "/location",
                    "/contact-us",
                ],
            ),
            (
                "team",
                [
                    "/team",
                    "/members",
                    "/ë³€í˜¸ì‚¬",
                    "/êµ¬ì„±ì›",
                    "/ì§ì›ì†Œê°œ",
                    "/íŒ€ì†Œê°œ",
                    "/lawyers",
                    "/attorneys",
                    "/staff",
                ],
            ),
        ]

        # ìƒí’ˆ í˜ì´ì§€ ì‹ë³„ íŒ¨í„´
        self.product_url_patterns = [
            r"/product/\d+",
            r"/goods/\d+",
            r"/item/\d+",
            r"/shop/\d+",
            r"/p/\d+",
            r"product_no=\d+",
            r"goods_no=\d+",
            r"item_id=\d+",
            r"/ìƒí’ˆ/\d+",
            r"/ì œí’ˆ/\d+",
        ]

    def discover_priority_urls(self) -> List[str]:
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ URL ë°œê²¬"""
        print("ğŸ” ìš°ì„ ìˆœìœ„ í˜ì´ì§€ ë°œê²¬ ì¤‘...")

        # 1. Sitemapì—ì„œ URL ìˆ˜ì§‘
        sitemap_urls = self.get_sitemap_urls()
        # print(f"ğŸ“‹ Sitemapì—ì„œ {len(sitemap_urls)}ê°œ URL ë°œê²¬")

        # 2. ë©”ì¸í˜ì´ì§€ì—ì„œ ë‚´ë¶€ ë§í¬ ìˆ˜ì§‘
        main_page_urls = self.get_internal_links_from_page(self.base_url)
        # print(f"ğŸ”— ë©”ì¸í˜ì´ì§€ì—ì„œ {len(main_page_urls)}ê°œ ë§í¬ ë°œê²¬")

        # 3. ì§ì ‘ ê²½ë¡œ í™•ì¸
        direct_urls = self.check_direct_paths()
        # print(f"ğŸ¯ ì§ì ‘ ê²½ë¡œì—ì„œ {len(direct_urls)}ê°œ URL í™•ì¸")

        # 4. ìƒí’ˆ í˜ì´ì§€ ìˆ˜ì§‘
        product_urls = self.discover_product_pages()
        # print(f"ğŸ›ï¸ ìƒí’ˆ í˜ì´ì§€ {len(product_urls)}ê°œ ë°œê²¬")

        # ëª¨ë“  URL í†µí•© ë° ë¶„ë¥˜
        all_urls = set(sitemap_urls + main_page_urls + direct_urls + product_urls)

        # ğŸš¨ ì•ˆì „ì¥ì¹˜: URLì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìµœì†Œí•œ ë©”ì¸ í˜ì´ì§€ë¼ë„ ì¶”ê°€
        if not all_urls:
            # print("âš ï¸ ë°œê²¬ëœ URLì´ ì—†ì–´ ë©”ì¸ í˜ì´ì§€ë¥¼ ê°•ì œ ì¶”ê°€í•©ë‹ˆë‹¤.")
            all_urls.add(self.base_url)

        # print(f"ğŸŒ ì´ {len(all_urls)}ê°œ ê³ ìœ  URL ìˆ˜ì§‘")

        priority_urls = self.categorize_and_prioritize_urls(list(all_urls))

        # ğŸš¨ ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ë¶„ë¥˜ í›„ì—ë„ URLì´ ì—†ìœ¼ë©´ ë©”ì¸ í˜ì´ì§€ ì¶”ê°€
        if not priority_urls:
            # print("âš ï¸ ë¶„ë¥˜ëœ URLì´ ì—†ì–´ ë©”ì¸ í˜ì´ì§€ë¥¼ ìµœì¢… ì¶”ê°€í•©ë‹ˆë‹¤.")
            priority_urls = [(self.base_url, "main")]

        final_urls = priority_urls[: self.max_pages]

        # print(f"\nğŸ¯ ìµœì¢… í¬ë¡¤ë§ ëŒ€ìƒ ({len(final_urls)}ê°œ):")
        for i, (url, page_type) in enumerate(final_urls, 1):
            print(f"  {i}. [{page_type.upper()}] {url}")

        return [url for url, _ in final_urls]

    def get_sitemap_urls(self) -> List[str]:
        """sitemap.xmlì—ì„œ URL ì¶”ì¶œ"""
        sitemap_candidates = [
            f"{self.base_url}/sitemap.xml",
            f"{self.base_url}/sitemap_index.xml",
            f"{self.base_url}/sitemaps/sitemap.xml",
            f"{self.base_url}/wp-sitemap.xml",
        ]

        urls: Set[str] = set()

        for sitemap_url in sitemap_candidates:
            try:
                response = requests.get(sitemap_url, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, "xml")
                    for loc in soup.find_all("loc"):
                        url = loc.get_text().strip()
                        if self.is_valid_internal_url(url):
                            urls.add(url)
            except Exception:
                continue

        return list(urls)

    def get_internal_links_from_page(self, url: str) -> List[str]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ë‚´ë¶€ ë§í¬ ì¶”ì¶œ"""
        if not SELENIUM_AVAILABLE:
            # print(f"âš ï¸ Selenium ì—†ì´ {url} ì²˜ë¦¬ ì¤‘...")
            return self._get_links_with_requests(url)

        driver = None
        urls: Set[str] = set()

        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.get(url)
            time.sleep(3)

            links = driver.find_elements(By.TAG_NAME, "a")
            for link in links:
                try:
                    href = link.get_attribute("href")
                    if href and self.is_valid_internal_url(href):
                        urls.add(href)
                except Exception:
                    continue

        except Exception as e:
            print(f"âš ï¸ {url}ì—ì„œ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # Selenium ì‹¤íŒ¨ì‹œ requestsë¡œ fallback
            return self._get_links_with_requests(url)
        finally:
            if driver:
                driver.quit()

        return list(urls)

    def _get_links_with_requests(self, url: str) -> List[str]:
        """requestsë¥¼ ì‚¬ìš©í•œ ë§í¬ ì¶”ì¶œ (Selenium ëŒ€ì²´)"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            links = soup.find_all("a", href=True)

            urls = set()
            for link in links:
                href = link.get("href")
                if href and self.is_valid_internal_url(href):
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href.startswith("/"):
                        href = self.base_url + href
                    elif not href.startswith("http"):
                        href = urljoin(url, href)
                    urls.add(href)

            return list(urls)
        except Exception as e:
            print(f"âš ï¸ requestsë¡œ {url} ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def check_direct_paths(self) -> List[str]:
        """ìš°ì„ ìˆœìœ„ ê²½ë¡œë“¤ì„ ì§ì ‘ í™•ì¸"""
        valid_urls = []

        for page_type, paths in self.priority_pages:
            if page_type == "product":  # ìƒí’ˆ í˜ì´ì§€ëŠ” ë³„ë„ ì²˜ë¦¬
                continue

            for path in paths:
                if path in ["", "/"]:
                    test_url = self.base_url
                else:
                    test_url = self.base_url + path

                if self.check_url_exists(test_url):
                    valid_urls.append(test_url)
                    # print(f"âœ… {page_type.upper()} í˜ì´ì§€ ë°œê²¬: {test_url}")

        return valid_urls

    def discover_product_pages(self) -> List[str]:
        """ìƒí’ˆ í˜ì´ì§€ URL ìˆ˜ì§‘"""
        print("ğŸ›ï¸ ìƒí’ˆ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")
        product_urls = set()

        # Seleniumì´ ì—†ì–´ë„ requestsë¡œ ìƒí’ˆ ë§í¬ ì°¾ê¸°
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            response = requests.get(self.base_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ìƒí’ˆ ë§í¬ ì„ íƒìë“¤ë¡œ ê²€ìƒ‰
            product_selectors = [
                'a[href*="/product/"]',
                'a[href*="/goods/"]',
                'a[href*="/item/"]',
                'a[href*="/shop/"]',
                'a[href*="/p/"]',
                'a[href*="product_no="]',
                'a[href*="goods_no="]',
                ".product-item a",
                ".goods-item a",
                ".item-link",
                ".product-link",
                '[class*="product"] a',
                '[class*="goods"] a',
                '[class*="item"] a',
            ]

            found_products = set()
            for selector in product_selectors:
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        href = element.get("href")
                        if href and self.is_product_url(href):
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if href.startswith("/"):
                                href = self.base_url + href
                            elif not href.startswith("http"):
                                href = urljoin(self.base_url, href)

                            if self.is_valid_internal_url(href):
                                found_products.add(href)
                                if len(found_products) >= self.max_product_pages * 2:
                                    break
                except Exception:
                    continue

                if len(found_products) >= self.max_product_pages * 2:
                    break

            product_urls.update(found_products)
            # print(f"   ë©”ì¸í˜ì´ì§€ì—ì„œ {len(found_products)}ê°œ ìƒí’ˆ ë§í¬ ë°œê²¬")

        except Exception as e:
            pass
            # print(f"âš ï¸ ë©”ì¸í˜ì´ì§€ ìƒí’ˆ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # ìœ íš¨ì„± ê²€ì¦ì„ ë” ê´€ëŒ€í•˜ê²Œ
        valid_products = []
        for url in list(product_urls)[: self.max_product_pages * 2]:
            # check_url_exists í˜¸ì¶œì„ ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬
            try:
                response = requests.head(url, timeout=5, allow_redirects=True)
                if response.status_code in [200, 301, 302]:
                    valid_products.append(url)
                    if len(valid_products) >= self.max_product_pages:
                        break
            except:
                # HEAD ìš”ì²­ ì‹¤íŒ¨ì‹œ GETìœ¼ë¡œ ì¬ì‹œë„
                try:
                    response = requests.get(url, timeout=5, allow_redirects=True)
                    if response.status_code in [200, 301, 302]:
                        valid_products.append(url)
                        if len(valid_products) >= self.max_product_pages:
                            break
                except:
                    continue

        # print(f"âœ… ìµœì¢… ìœ íš¨í•œ ìƒí’ˆ í˜ì´ì§€: {len(valid_products)}ê°œ")
        return valid_products

    def categorize_and_prioritize_urls(self, urls: List[str]) -> List[tuple]:
        """URLì„ ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ì •ë ¬"""
        categorized = {}

        # ê° ìš°ì„ ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™”
        for page_type, _ in self.priority_pages:
            categorized[page_type] = []
        categorized["other"] = []

        # URL ë¶„ë¥˜
        for url in urls:
            path = urlparse(url).path.lower()
            query = urlparse(url).query.lower()
            full_url_lower = url.lower()
            categorized_flag = False

            # ìƒí’ˆ í˜ì´ì§€ ìš°ì„  í™•ì¸
            if self.is_product_url(url):
                categorized["product"].append(url)
                categorized_flag = True
            else:
                # ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ë¶„ë¥˜
                for page_type, keywords in self.priority_pages:
                    if page_type == "product":  # ìƒí’ˆì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬
                        continue
                    if self.matches_page_type(path, query, full_url_lower, keywords):
                        categorized[page_type].append(url)
                        categorized_flag = True
                        break

            if not categorized_flag:
                categorized["other"].append(url)

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ìµœì ì˜ URL ì„ íƒ
        final_urls = []

        for page_type, _ in self.priority_pages:
            urls_in_category = categorized[page_type]
            if urls_in_category:
                if page_type == "product":
                    # ìƒí’ˆ í˜ì´ì§€ëŠ” ìµœëŒ€ 3ê°œ ì„ íƒ
                    selected_products = urls_in_category[: self.max_product_pages]
                    for product_url in selected_products:
                        final_urls.append((product_url, page_type))
                else:
                    # ë‹¤ë¥¸ í˜ì´ì§€ëŠ” 1ê°œì”©
                    best_url = self.select_best_url(urls_in_category, page_type)
                    final_urls.append((best_url, page_type))

        # ê¸°íƒ€ URL ì¶”ê°€ (ë‚¨ì€ ìŠ¬ë¡¯ì´ ìˆë‹¤ë©´)
        remaining_slots = self.max_pages - len(final_urls)
        if remaining_slots > 0 and categorized["other"]:
            for url in categorized["other"][:remaining_slots]:
                final_urls.append((url, "other"))

        return final_urls

    def is_product_url(self, url: str) -> bool:
        """ìƒí’ˆ í˜ì´ì§€ URLì¸ì§€ í™•ì¸"""
        for pattern in self.product_url_patterns:
            if re.search(pattern, url, re.I):
                return True
        return False

    def matches_page_type(
        self, path: str, query: str, full_url: str, keywords: List[str]
    ) -> bool:
        """ê²½ë¡œê°€ íŠ¹ì • í˜ì´ì§€ íƒ€ì…ì˜ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸"""
        # ë©”ì¸í˜ì´ì§€ íŠ¹ë³„ ì²˜ë¦¬
        if "" in keywords or "/" in keywords:
            if path in [
                "",
                "/",
                "/index",
                "/index.html",
                "/home",
                "/main",
            ] or path.endswith("/index"):
                return True

        # ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ í™•ì¸
        for keyword in keywords:
            if keyword in ["", "/"]:
                continue
            if keyword in path or keyword in query or keyword in full_url:
                return True

        return False

    def select_best_url(self, urls: List[str], page_type: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°€ì¥ ì ì ˆí•œ URL ì„ íƒ"""
        if not urls:
            return ""
        if len(urls) == 1:
            return urls[0]

        # ê°„ë‹¨í•œ ë¡œì§: ê°€ì¥ ì§§ì€ URL ì„ íƒ
        return min(urls, key=len)

    def check_url_exists(self, url: str) -> bool:
        """URLì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ë” ê´€ëŒ€í•œ ê²€ì¦)"""
        try:
            # HEAD ìš”ì²­ ë¨¼ì € ì‹œë„
            response = requests.head(url, timeout=8, allow_redirects=True)
            if response.status_code in [
                200,
                301,
                302,
                403,
            ]:  # 403ë„ í—ˆìš© (ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ HEAD ì°¨ë‹¨)
                return True
        except Exception:
            pass

        try:
            # GET ìš”ì²­ìœ¼ë¡œ ì¬ì‹œë„
            response = requests.get(url, timeout=8, allow_redirects=True)
            if response.status_code in [200, 301, 302]:
                return True
        except Exception:
            pass

        # ê¸°ë³¸ í˜ì´ì§€ë“¤ì€ ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬
        path = urlparse(url).path.lower()
        if path in ["", "/", "/index", "/index.html", "/home", "/main"]:
            return True

        return False

    def is_valid_internal_url(self, url: str) -> bool:
        """ë‚´ë¶€ URLì¸ì§€ ê²€ì¦ (ë” ê´€ëŒ€í•œ ë²„ì „)"""
        try:
            if not url or url.strip() == "":
                return False

            # ì ˆëŒ€ URLì¸ ê²½ìš° ë„ë©”ì¸ ì²´í¬
            if url.startswith("http"):
                parsed = urlparse(url)
                if parsed.netloc and parsed.netloc != self.parsed_base.netloc:
                    return False

            # ìƒëŒ€ URLì¸ ê²½ìš° ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
            elif url.startswith("/"):
                return True

            # ì œì™¸í•  í™•ì¥ì
            excluded_extensions = [
                ".pdf",
                ".jpg",
                ".jpeg",
                ".png",
                ".gif",
                ".zip",
                ".doc",
                ".docx",
            ]
            if any(url.lower().endswith(ext) for ext in excluded_extensions):
                return False

            # ì œì™¸í•  í”„ë¡œí† ì½œ
            excluded_patterns = [
                "javascript:",
                "mailto:",
                "tel:",
                "#",
                "data:",
                "blob:",
            ]
            if any(url.lower().startswith(pattern) for pattern in excluded_patterns):
                return False

            return True
        except Exception:
            return False

    def crawl_single_page(
        self, url: str, page_type: str = None
    ) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ í˜ì´ì§€ HTML ìˆ˜ì§‘"""
        if url in self.crawled_urls:
            return None

        # page_typeì´ ì œê³µë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°ì§€
        if page_type is None:
            page_type = self.detect_page_type(url)

        print(f"ğŸ” í¬ë¡¤ë§ ì¤‘")
        # print(f"ğŸ” [{page_type.upper()}] í¬ë¡¤ë§ ì¤‘: {url}")

        if not SELENIUM_AVAILABLE:
            return self._crawl_with_requests(url, page_type)

        driver = None
        try:
            driver = webdriver.Chrome(options=self.chrome_options)
            driver.get(url)

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(3)

            # í˜ì´ì§€ HTML ì „ì²´ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source

            # HTML íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{page_type}_{timestamp}.html"
            filepath = f"outputs/{filename}"

            with open(filename, "w", encoding="utf-8") as f:
                f.write(page_source)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "url": url,
                "page_type": page_type,
                "crawl_timestamp": datetime.now().isoformat(),
                "filename": filepath,
                "file_size": len(page_source.encode("utf-8")),
                "html_content": page_source,  # ë¶„ì„ìš©ìœ¼ë¡œ HTML í¬í•¨
            }

            print(f"âœ… í¬ë¡¤ë§ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            self.crawled_urls.add(url)
            return metadata

        except Exception as e:
            print(f"âŒ [{page_type.upper()}] Selenium í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return self._crawl_with_requests(url, page_type)
        finally:
            if driver:
                driver.quit()

    def _crawl_with_requests(
        self, url: str, page_type: str
    ) -> Optional[Dict[str, Any]]:
        """requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ (Selenium ëŒ€ì²´)"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            page_source = response.text

            # HTML íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{page_type}_{timestamp}.html"
            filepath = f"outputs/{filename}"

            with open(filename, "w", encoding="utf-8") as f:
                f.write(page_source)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "url": url,
                "page_type": page_type,
                "crawl_timestamp": datetime.now().isoformat(),
                "filename": filepath,
                "file_size": len(page_source.encode("utf-8")),
                "html_content": page_source,
            }

            # print(f"âœ… [{page_type.upper()}] requestsë¡œ ì €ì¥ ì™„ë£Œ: {filename}")
            self.crawled_urls.add(url)
            return metadata

        except Exception as e:
            print(f"âŒ [{page_type.upper()}] requests í¬ë¡¤ë§ë„ ì‹¤íŒ¨ {url}: {e}")
            return None

    def detect_page_type(self, url: str) -> str:
        """í˜ì´ì§€ íƒ€ì… ê°ì§€"""
        if self.is_product_url(url):
            return "product"

        path = urlparse(url).path.lower()
        query = urlparse(url).query.lower()
        full_url = url.lower()

        for page_type, keywords in self.priority_pages:
            if page_type == "product":
                continue
            if self.matches_page_type(path, query, full_url, keywords):
                return page_type

        return "other"

    def crawl_site(self) -> List[Dict[str, Any]]:
        """ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸš€ {self.base_url}  í¬ë¡¤ë§ ì‹œì‘")  # ë‹¤ì¤‘ í˜ì´ì§€
        # print(f"ğŸ“Š ìµœëŒ€ {self.max_pages}ê°œ í˜ì´ì§€ (ìƒí’ˆ {self.max_product_pages}ê°œ í¬í•¨)")

        # URL ë°œê²¬ ë° í¬ë¡¤ë§
        urls_to_crawl = self.discover_priority_urls()

        if not urls_to_crawl:
            print("âŒ í¬ë¡¤ë§í•  URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # print(f"\nğŸ“‹ í¬ë¡¤ë§ ì‹œì‘ - {len(urls_to_crawl)}ê°œ í˜ì´ì§€")
        print(f"\nğŸ“‹ í¬ë¡¤ë§ ì‹œì‘")

        # ê° URL í¬ë¡¤ë§
        for i, url in enumerate(urls_to_crawl, 1):
            # print(f"\n{'='*50}")
            # print(f"ì§„í–‰ë¥ : {i}/{len(urls_to_crawl)}")
            result = self.crawl_single_page(url)
            if result:
                self.results.append(result)

            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            if i < len(urls_to_crawl):
                time.sleep(2)

        # print(f"\nğŸ‰ ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(self.results)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘")
        print(f"\nğŸ‰ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
        return self.results


# ===== @tool í•¨ìˆ˜ë“¤ =====
@tool
def crawl_full_website(base_url: str, max_pages: int = 9) -> dict:
    """
    sitemap.xml ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    Args:
        base_url: í¬ë¡¤ë§í•  ê¸°ë³¸ URL
        max_pages: ìµœëŒ€ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

    Returns:
        í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        crawler = StreamlinedSEOGEOCrawler(base_url, max_pages)
        crawl_results = crawler.crawl_site()

        if not crawl_results:
            return {
                "error": "í¬ë¡¤ë§ëœ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤",
                "base_url": base_url,
                "pages_crawled": 0,
            }

        # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„ë¥˜
        page_types = {}
        for result in crawl_results:
            page_type = result["page_type"]
            page_types[page_type] = page_types.get(page_type, 0) + 1

        return {
            "base_url": base_url,
            "pages_crawled": len(crawl_results),
            "crawl_results": crawl_results,
            "page_types": page_types,
            "success": True,
            "crawl_timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        return {
            "error": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}",
            "base_url": base_url,
            "pages_crawled": 0,
            "success": False,
        }


@tool
def analyze_page_from_html(
    html_content: str, url: str, page_type: str = "unknown"
) -> dict:
    """
    HTML ì½˜í…ì¸ ë¡œë¶€í„° ê°œë³„ í˜ì´ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    Args:
        html_content: ë¶„ì„í•  HTML ì½˜í…ì¸ 
        url: í˜ì´ì§€ URL
        page_type: í˜ì´ì§€ íƒ€ì…

    Returns:
        í˜ì´ì§€ ë¶„ì„ ê²°ê³¼
    """
    try:
        soup = BeautifulSoup(html_content, "html.parser")

        # SEO ìš”ì†Œ ë¶„ì„
        analysis = {
            "url": url,
            "page_type": page_type,
            "title": {
                "content": (
                    soup.find("title").get_text().strip() if soup.find("title") else ""
                ),
                "length": (
                    len(soup.find("title").get_text().strip())
                    if soup.find("title")
                    else 0
                ),
                "good": False,
            },
            "meta_description": {"content": "", "length": 0, "good": False},
            "headings": {
                "h1_count": len(soup.find_all("h1")),
                "h1_content": [h1.get_text().strip() for h1 in soup.find_all("h1")],
                "h2_count": len(soup.find_all("h2")),
                "good": False,
            },
            "images": {
                "total": len(soup.find_all("img")),
                "without_alt": 0,
                "good": False,
            },
            "links": {"internal": 0, "external": 0, "good": False},
            "structured_data": {
                "json_ld_count": len(
                    soup.find_all("script", {"type": "application/ld+json"})
                ),
                "good": False,
            },
            "content_quality": {
                "word_count": len(soup.get_text().split()),
                "paragraph_count": len(soup.find_all("p")),
                "good": False,
            },
        }

        # Meta Description ë¶„ì„
        meta_desc = soup.find("meta", {"name": "description"})
        if meta_desc and meta_desc.get("content"):
            desc_content = meta_desc.get("content").strip()
            analysis["meta_description"]["content"] = desc_content
            analysis["meta_description"]["length"] = len(desc_content)
            analysis["meta_description"]["good"] = 120 <= len(desc_content) <= 160

        # Title í’ˆì§ˆ í‰ê°€
        title_length = analysis["title"]["length"]
        analysis["title"]["good"] = 30 <= title_length <= 60

        # Headings í’ˆì§ˆ í‰ê°€
        analysis["headings"]["good"] = analysis["headings"]["h1_count"] == 1

        # Images Alt í…ìŠ¤íŠ¸ í™•ì¸
        images_without_alt = len(
            [img for img in soup.find_all("img") if not img.get("alt")]
        )
        analysis["images"]["without_alt"] = images_without_alt
        analysis["images"]["good"] = (
            images_without_alt == 0 if analysis["images"]["total"] > 0 else True
        )

        # ë§í¬ ë¶„ì„
        domain = urlparse(url).netloc
        links = soup.find_all("a", href=True)
        internal_links = 0
        external_links = 0

        for link in links:
            href = link.get("href")
            if href.startswith("http"):
                if domain in href:
                    internal_links += 1
                else:
                    external_links += 1
            elif href.startswith("/"):
                internal_links += 1

        analysis["links"]["internal"] = internal_links
        analysis["links"]["external"] = external_links
        analysis["links"]["good"] = internal_links > 0

        # êµ¬ì¡°í™”ëœ ë°ì´í„° í‰ê°€
        analysis["structured_data"]["good"] = (
            analysis["structured_data"]["json_ld_count"] > 0
        )

        # ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
        word_count = analysis["content_quality"]["word_count"]
        analysis["content_quality"]["good"] = word_count >= 300

        # ê°œë³„ í˜ì´ì§€ SEO ì ìˆ˜ ê³„ì‚°
        score_factors = [
            analysis["title"]["good"],
            analysis["meta_description"]["good"],
            analysis["headings"]["good"],
            analysis["images"]["good"],
            analysis["links"]["good"],
            analysis["structured_data"]["good"],
            analysis["content_quality"]["good"],
        ]

        seo_score = (sum(score_factors) / len(score_factors)) * 100
        analysis["seo_score"] = round(seo_score, 1)

        return analysis

    except Exception as e:
        return {
            "error": f"í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
            "url": url,
            "page_type": page_type,
            "seo_score": 0,
        }


@tool
def calculate_site_geo_scores(crawl_results: list) -> dict:
    """
    í¬ë¡¤ë§ëœ ì „ì²´ ì‚¬ì´íŠ¸ì˜ GEO ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        crawl_results: í¬ë¡¤ë§ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì‚¬ì´íŠ¸ ì „ì²´ GEO ì ìˆ˜
    """
    try:
        total_pages = len(crawl_results)
        if total_pages == 0:
            return {"error": "ë¶„ì„í•  í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # ê° í˜ì´ì§€ë³„ GEO ì ìˆ˜ ê³„ì‚°
        page_geo_scores = []

        for result in crawl_results:
            html_content = result.get("html_content", "")
            url = result.get("url", "")

            if not html_content:
                continue

            soup = BeautifulSoup(html_content, "html.parser")
            page_text = soup.get_text()

            # GEO 6ê°€ì§€ ê¸°ì¤€ í‰ê°€
            geo_scores = {
                "clarity": evaluate_clarity(soup, page_text)["score"],
                "structure": evaluate_structure(soup, page_text)["score"],
                "context": evaluate_context(soup, page_text)["score"],
                "alignment": evaluate_alignment(soup, page_text)["score"],
                "timeliness": evaluate_timeliness(soup, page_text)["score"],
                "originality": evaluate_originality(soup, page_text)["score"],
            }

            page_total = sum(geo_scores.values()) / len(geo_scores)

            page_geo_scores.append(
                {
                    "url": url,
                    "page_type": result.get("page_type", "unknown"),
                    "scores": geo_scores,
                    "total_score": round(page_total, 1),
                }
            )

        # ì‚¬ì´íŠ¸ ì „ì²´ í‰ê·  ê³„ì‚°
        if not page_geo_scores:
            return {"error": "GEO ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤"}

        site_averages = {
            "clarity": sum(p["scores"]["clarity"] for p in page_geo_scores)
            / len(page_geo_scores),
            "structure": sum(p["scores"]["structure"] for p in page_geo_scores)
            / len(page_geo_scores),
            "context": sum(p["scores"]["context"] for p in page_geo_scores)
            / len(page_geo_scores),
            "alignment": sum(p["scores"]["alignment"] for p in page_geo_scores)
            / len(page_geo_scores),
            "timeliness": sum(p["scores"]["timeliness"] for p in page_geo_scores)
            / len(page_geo_scores),
            "originality": sum(p["scores"]["originality"] for p in page_geo_scores)
            / len(page_geo_scores),
        }

        site_total_geo = sum(site_averages.values()) / len(site_averages)

        return {
            "site_geo_score": round(site_total_geo, 1),
            "site_averages": {k: round(v, 1) for k, v in site_averages.items()},
            "page_scores": page_geo_scores,
            "pages_analyzed": len(page_geo_scores),
            "recommendations": generate_site_geo_recommendations(site_averages),
        }

    except Exception as e:
        return {"error": f"ì‚¬ì´íŠ¸ GEO ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}", "site_geo_score": 0}


@tool
def calculate_site_average_scores(page_analyses: list) -> dict:
    """
    ê°œë³„ í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ì´íŠ¸ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        page_analyses: í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ê²°ê³¼
    """
    try:
        if not page_analyses:
            return {"error": "ë¶„ì„í•  í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤"}

        valid_analyses = [
            p for p in page_analyses if not p.get("error") and p.get("seo_score", 0) > 0
        ]

        if not valid_analyses:
            return {"error": "ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_seo_score = sum(p["seo_score"] for p in valid_analyses) / len(
            valid_analyses
        )

        # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„ë¥˜
        page_types = {}
        for analysis in valid_analyses:
            page_type = analysis.get("page_type", "unknown")
            if page_type not in page_types:
                page_types[page_type] = []
            page_types[page_type].append(analysis)

        # í˜ì´ì§€ íƒ€ì…ë³„ í‰ê·  ì ìˆ˜
        type_averages = {}
        for page_type, analyses in page_types.items():
            type_avg = sum(a["seo_score"] for a in analyses) / len(analyses)
            type_averages[page_type] = round(type_avg, 1)

        # ì¢…í•© ë¬¸ì œì  ë¶„ì„
        common_issues = analyze_common_issues(valid_analyses)

        # ìš°ì„ ìˆœìœ„ ê°œì„  í•­ëª©
        priority_improvements = generate_priority_improvements(
            valid_analyses, common_issues
        )

        return {
            "site_average_seo_score": round(avg_seo_score, 1),
            "pages_analyzed": len(valid_analyses),
            "page_type_averages": type_averages,
            "page_analyses": valid_analyses,
            "common_issues": common_issues,
            "priority_improvements": priority_improvements,
            "success": True,
        }

    except Exception as e:
        return {"error": f"ì‚¬ì´íŠ¸ í‰ê·  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}", "success": False}


# ===== GEO í‰ê°€ í•¨ìˆ˜ë“¤ =====
def evaluate_clarity(soup: BeautifulSoup, page_text: str) -> dict:
    """ëª…í™•ì„± í‰ê°€"""
    score = 0
    details = {"specific_facts": 0, "clear_statements": 0, "contact_info": 0}

    # êµ¬ì²´ì ì¸ ì‚¬ì‹¤/ìˆ˜ì¹˜ í™•ì¸
    numeric_patterns = re.findall(r"\d+[%ë…„ë„ì›ë‹¬ëŸ¬$â‚¬â‚©]|\d+\.\d+", page_text)
    details["specific_facts"] = len(numeric_patterns)
    if len(numeric_patterns) >= 10:
        score += 30
    elif len(numeric_patterns) >= 5:
        score += 20
    elif len(numeric_patterns) >= 1:
        score += 10

    # ëª…í™•í•œ ì§„ìˆ ë¬¸ í™•ì¸
    clear_indicators = ["ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ì œê³µí•©ë‹ˆë‹¤", "ì „ë¬¸", "ìµœê³ "]
    clear_count = sum(
        page_text.lower().count(indicator) for indicator in clear_indicators
    )
    details["clear_statements"] = clear_count
    if clear_count >= 20:
        score += 25
    elif clear_count >= 10:
        score += 15
    elif clear_count >= 5:
        score += 10

    # ì—°ë½ì²˜ ì •ë³´
    contact_patterns = [
        r"(\d{2,3}-\d{3,4}-\d{4})",
        r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})",
    ]
    contact_count = sum(
        len(re.findall(pattern, page_text)) for pattern in contact_patterns
    )
    details["contact_info"] = contact_count
    if contact_count >= 2:
        score += 45
    elif contact_count >= 1:
        score += 25

    return {"score": min(score, 100), "details": details}


def evaluate_structure(soup: BeautifulSoup, page_text: str) -> dict:
    """êµ¬ì¡°ì„± í‰ê°€"""
    score = 0
    details = {"headings": 0, "lists": 0, "schema": 0}

    # í—¤ë”© êµ¬ì¡°
    h_tags = len(soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]))
    details["headings"] = h_tags
    if h_tags >= 5:
        score += 30
    elif h_tags >= 3:
        score += 20
    elif h_tags >= 1:
        score += 10

    # ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
    lists = len(soup.find_all(["ul", "ol", "dl"]))
    details["lists"] = lists
    if lists >= 3:
        score += 25
    elif lists >= 1:
        score += 15

    # Schema ë§ˆí¬ì—…
    schema_count = len(soup.find_all("script", {"type": "application/ld+json"}))
    details["schema"] = schema_count
    if schema_count >= 1:
        score += 45

    return {"score": min(score, 100), "details": details}


def evaluate_context(soup: BeautifulSoup, page_text: str) -> dict:
    """ë§¥ë½ì„± í‰ê°€"""
    score = 0
    details = {"word_count": 0, "topics": 0}

    word_count = len(page_text.split())
    details["word_count"] = word_count
    if word_count >= 1000:
        score += 40
    elif word_count >= 500:
        score += 25
    elif word_count >= 200:
        score += 15

    # ì£¼ì œ ë‹¤ì–‘ì„±
    paragraphs = soup.find_all("p")
    unique_topics = set()
    for p in paragraphs:
        words = re.findall(r"[ê°€-í£]{2,}", p.get_text())
        unique_topics.update(words)

    details["topics"] = len(unique_topics)
    if len(unique_topics) >= 50:
        score += 35
    elif len(unique_topics) >= 25:
        score += 20
    elif len(unique_topics) >= 10:
        score += 10

    # ë©€í‹°ë¯¸ë””ì–´
    multimedia = len(soup.find_all(["img", "video", "iframe"]))
    if multimedia >= 5:
        score += 25
    elif multimedia >= 1:
        score += 15

    return {"score": min(score, 100), "details": details}


def evaluate_alignment(soup: BeautifulSoup, page_text: str) -> dict:
    """ì •í•©ì„± í‰ê°€"""
    score = 0
    details = {"alt_quality": 0, "consistency": 0}

    # ì´ë¯¸ì§€ ALT í’ˆì§ˆ
    images = soup.find_all("img")
    quality_alt = sum(
        1 for img in images if img.get("alt") and len(img.get("alt", "")) >= 5
    )
    details["alt_quality"] = quality_alt
    if images and quality_alt / len(images) >= 0.8:
        score += 50
    elif images and quality_alt / len(images) >= 0.5:
        score += 30

    # ìš©ì–´ ì¼ê´€ì„±
    title = soup.find("title")
    h1 = soup.find("h1")
    if title and h1:
        title_words = set(title.get_text().lower().split())
        h1_words = set(h1.get_text().lower().split())
        if title_words.intersection(h1_words):
            score += 50
            details["consistency"] = 1

    return {"score": min(score, 100), "details": details}


def evaluate_timeliness(soup: BeautifulSoup, page_text: str) -> dict:
    """ì‹œì˜ì„± í‰ê°€"""
    score = 0
    details = {"dates": 0, "recent_keywords": 0}

    # ë‚ ì§œ ë° ìµœì‹ ì„± í‚¤ì›Œë“œ
    date_patterns = [r"20\d{2}ë…„", r"ìµœê·¼", r"ìµœì‹ ", r"ìƒˆë¡œìš´"]
    date_count = sum(
        len(re.findall(pattern, page_text, re.I)) for pattern in date_patterns
    )
    details["dates"] = date_count
    if date_count >= 10:
        score += 60
    elif date_count >= 5:
        score += 40
    elif date_count >= 1:
        score += 20

    # ì‹œì˜ì„± í‚¤ì›Œë“œ
    current_keywords = ["í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ì´ë²ˆ", "íŠ¸ë Œë“œ"]
    current_count = sum(
        page_text.lower().count(keyword) for keyword in current_keywords
    )
    details["recent_keywords"] = current_count
    if current_count >= 5:
        score += 40
    elif current_count >= 2:
        score += 20

    return {"score": min(score, 100), "details": details}


def evaluate_originality(soup: BeautifulSoup, page_text: str) -> dict:
    """ë…ì°½ì„± í‰ê°€"""
    score = 0
    details = {"insights": 0, "data": 0}

    # ë…ì°½ì  í†µì°°
    insight_keywords = ["ê²½í—˜ìƒ", "ë¶„ì„í•˜ë©´", "ë°œê²¬í–ˆìŠµë‹ˆë‹¤", "ì—°êµ¬"]
    insight_count = sum(
        page_text.lower().count(keyword) for keyword in insight_keywords
    )
    details["insights"] = insight_count
    if insight_count >= 5:
        score += 50
    elif insight_count >= 2:
        score += 30
    elif insight_count >= 1:
        score += 15

    # ê³ ìœ  ë°ì´í„°/í†µê³„
    data_indicators = ["ì¡°ì‚¬", "í†µê³„", "ë°ì´í„°", "ìì²´"]
    data_count = sum(page_text.lower().count(keyword) for keyword in data_indicators)
    details["data"] = data_count
    if data_count >= 5:
        score += 50
    elif data_count >= 2:
        score += 30
    elif data_count >= 1:
        score += 15

    return {"score": min(score, 100), "details": details}


def generate_site_geo_recommendations(site_averages: dict) -> list:
    """ì‚¬ì´íŠ¸ ì „ì²´ GEO ì¶”ì²œì‚¬í•­ ìƒì„±"""
    recommendations = []

    for criterion, score in site_averages.items():
        if score < 60:
            if criterion == "clarity":
                recommendations.append("ì „ í˜ì´ì§€ì— êµ¬ì²´ì  ì •ë³´ì™€ ì—°ë½ì²˜ ë³´ê°• í•„ìš”")
            elif criterion == "structure":
                recommendations.append("ì‚¬ì´íŠ¸ ì „ì²´ í—¤ë”© êµ¬ì¡°ì™€ Schema ë§ˆí¬ì—… ê°œì„ ")
            elif criterion == "context":
                recommendations.append("ëª¨ë“  í˜ì´ì§€ì˜ ì½˜í…ì¸  ë¶„ëŸ‰ê³¼ ê¹Šì´ í™•ëŒ€")
            elif criterion == "alignment":
                recommendations.append("ì´ë¯¸ì§€ ALT ì†ì„±ê³¼ ìš©ì–´ ì¼ê´€ì„± ì „ì‚¬ì  ê°œì„ ")
            elif criterion == "timeliness":
                recommendations.append("ì‚¬ì´íŠ¸ ì „ì²´ ìµœì‹  ì •ë³´ ì—…ë°ì´íŠ¸")
            elif criterion == "originality":
                recommendations.append("ë…ì°½ì  ì½˜í…ì¸ ì™€ ê³ ìœ  ë°ì´í„° ì „ í˜ì´ì§€ í™•ëŒ€")

    return recommendations


def analyze_common_issues(page_analyses: list) -> dict:
    """í˜ì´ì§€ë“¤ì˜ ê³µí†µ ë¬¸ì œì  ë¶„ì„"""
    issues = {
        "title_issues": 0,
        "description_issues": 0,
        "h1_issues": 0,
        "alt_issues": 0,
        "schema_issues": 0,
        "content_issues": 0,
    }

    total_pages = len(page_analyses)

    for analysis in page_analyses:
        if not analysis.get("title", {}).get("good"):
            issues["title_issues"] += 1
        if not analysis.get("meta_description", {}).get("good"):
            issues["description_issues"] += 1
        if not analysis.get("headings", {}).get("good"):
            issues["h1_issues"] += 1
        if not analysis.get("images", {}).get("good"):
            issues["alt_issues"] += 1
        if not analysis.get("structured_data", {}).get("good"):
            issues["schema_issues"] += 1
        if not analysis.get("content_quality", {}).get("good"):
            issues["content_issues"] += 1

    # ë¹„ìœ¨ë¡œ ë³€í™˜
    for issue_type in issues:
        issues[issue_type] = round((issues[issue_type] / total_pages) * 100, 1)

    return issues


def generate_priority_improvements(page_analyses: list, common_issues: dict) -> list:
    """ìš°ì„ ìˆœìœ„ ê°œì„  í•­ëª© ìƒì„±"""
    improvements = []

    # 80% ì´ìƒì˜ í˜ì´ì§€ì— ë¬¸ì œê°€ ìˆëŠ” í•­ëª©ì„ ìš°ì„ ìˆœìœ„ë¡œ
    if common_issues["title_issues"] >= 80:
        improvements.append(
            {
                "priority": "ë†’ìŒ",
                "issue": "Title íƒœê·¸ ìµœì í™”",
                "affected_pages": f"{common_issues['title_issues']}%",
                "impact": "ê²€ìƒ‰ ê²°ê³¼ í´ë¦­ë¥  20-30% í–¥ìƒ",
            }
        )

    if common_issues["description_issues"] >= 80:
        improvements.append(
            {
                "priority": "ë†’ìŒ",
                "issue": "Meta Description ê°œì„ ",
                "affected_pages": f"{common_issues['description_issues']}%",
                "impact": "ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ í’ˆì§ˆ ê°œì„ ",
            }
        )

    if common_issues["schema_issues"] >= 60:
        improvements.append(
            {
                "priority": "ì¤‘ê°„",
                "issue": "êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ê°€",
                "affected_pages": f"{common_issues['schema_issues']}%",
                "impact": "ë¦¬ì¹˜ ìŠ¤ë‹ˆí« ë° AI ê²€ìƒ‰ ëŒ€ì‘",
            }
        )

    if common_issues["h1_issues"] >= 70:
        improvements.append(
            {
                "priority": "ì¤‘ê°„",
                "issue": "H1 íƒœê·¸ êµ¬ì¡° ì •ë¦¬",
                "affected_pages": f"{common_issues['h1_issues']}%",
                "impact": "í˜ì´ì§€ êµ¬ì¡° ë° ì ‘ê·¼ì„± ê°œì„ ",
            }
        )

    if common_issues["alt_issues"] >= 50:
        improvements.append(
            {
                "priority": "ë‚®ìŒ",
                "issue": "ì´ë¯¸ì§€ ALT ì†ì„± ì¶”ê°€",
                "affected_pages": f"{common_issues['alt_issues']}%",
                "impact": "ì ‘ê·¼ì„± ë° ì´ë¯¸ì§€ ê²€ìƒ‰ ê°œì„ ",
            }
        )

    return improvements


# ===== JSON-LD ìƒì„± í•¨ìˆ˜ë“¤ =====
def extract_page_data_from_analysis(
    seo_analysis: dict, url: str, page_type: str
) -> dict:
    """ì´ë¯¸ ë¶„ì„ëœ ë°ì´í„°ì—ì„œ JSON-LDìš© ë°ì´í„° ì¶”ì¶œ"""
    try:
        # ì´ë¯¸ ë¶„ì„ëœ ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        title_data = seo_analysis.get("title", {})
        desc_data = seo_analysis.get("meta_description", {})
        h1_data = seo_analysis.get("headings", {})

        company_name = title_data.get("content", "")
        description = desc_data.get("content", "")

        # H1ì—ì„œ ì¶”ê°€ ì •ë³´ (ì´ë¯¸ ì¶”ì¶œë¨)
        h1_contents = h1_data.get("h1_content", [])
        if h1_contents and not company_name:
            company_name = h1_contents[0]

        # ê°„ë‹¨í•œ ì •ë¦¬
        if company_name:
            # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±° (ì˜ˆ: "- í™ˆí˜ì´ì§€", "| íšŒì‚¬ëª…" ë“±)
            company_name = re.sub(r"\s*[-|]\s*.*$", "", company_name).strip()

        return {
            "company_name": company_name or "ì›¹ì‚¬ì´íŠ¸",
            "description": description or "ë¹„ì¦ˆë‹ˆìŠ¤ ì›¹ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.",
            "url": url,
            "page_type": page_type,
            "h1_content": h1_contents[0] if h1_contents else "",
        }

    except Exception as e:
        return {
            "company_name": "ì›¹ì‚¬ì´íŠ¸",
            "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ì›¹ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.",
            "url": url,
            "page_type": page_type,
            "h1_content": "",
        }


def generate_template_schema(
    business_type: str, page_type: str, page_data: dict
) -> dict:
    """í…œí”Œë¦¿ ê¸°ë°˜ JSON-LD ìƒì„±"""
    try:
        # ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì… ë§¤í•‘
        template_key = business_type
        if business_type not in BUSINESS_SCHEMA_TEMPLATES:
            template_key = "default"

        # í˜ì´ì§€ íƒ€ì…ë³„ í…œí”Œë¦¿ ì„ íƒ
        templates = BUSINESS_SCHEMA_TEMPLATES[template_key]
        if page_type not in templates:
            if "main" in templates:
                page_type = "main"  # ê¸°ë³¸ê°’
            else:
                page_type = list(templates.keys())[0]  # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿

        # í…œí”Œë¦¿ ë³µì‚¬ ë° ë°ì´í„° ì¹˜í™˜
        schema_template = json.loads(json.dumps(templates[page_type]))

        # ë¬¸ìì—´ ì¹˜í™˜
        schema_json = json.dumps(schema_template, ensure_ascii=False)
        for key, value in page_data.items():
            placeholder = "{" + key + "}"
            schema_json = schema_json.replace(placeholder, str(value))

        # ë‚¨ì€ placeholder ì œê±°
        import re

        schema_json = re.sub(r"\{[^}]+\}", "ì •ë³´ì—†ìŒ", schema_json)

        schema = json.loads(schema_json)

        html_schema = f'<script type="application/ld+json">\n{json.dumps(schema, ensure_ascii=False, indent=2)}\n</script>'

        return {
            "generated_schemas": [schema],
            "schema_types": [schema.get("@type", "Organization")],
            "html_schemas": [html_schema],
            "combined_html": html_schema,
            "extraction_notes": f"í…œí”Œë¦¿ ê¸°ë°˜ {business_type} {page_type} ìŠ¤í‚¤ë§ˆ",
            "generation_method": "template_based",
            "success": True,
        }

    except Exception as e:
        # print(f"      âŒ í…œí”Œë¦¿ ìŠ¤í‚¤ë§ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        return generate_fallback_jsonld(
            business_type,
            page_type,
            page_data.get("company_name", ""),
            page_data.get("description", ""),
        )


def generate_fallback_jsonld(
    business_type: str, page_type: str, title: str, description: str
) -> dict:
    """LLM ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ JSON-LD ë°˜í™˜"""

    # ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…ë³„ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ë§¤í•‘
    business_schema_mapping = {
        "ë²•ë¬´ë²•ì¸": "LegalService",
        "ë³‘ì›": "MedicalOrganization",
        "ì¹´í˜": "CafeOrCoffeeShop",
        "ITíšŒì‚¬": "Corporation",
        "ì‡¼í•‘ëª°": "OnlineStore",
        "ì˜¨ë¼ì¸ì‡¼í•‘ëª°": "OnlineStore",
        "ì´ì»¤ë¨¸ìŠ¤": "OnlineStore",
        "êµìœ¡ê¸°ê´€": "EducationalOrganization",
        "ë¶€ë™ì‚°": "RealEstateAgent",
        "ë¯¸ìš©ì‹¤": "BeautySalon",
    }

    schema_type = business_schema_mapping.get(business_type, "Organization")

    basic_schema = {
        "@context": "https://schema.org",
        "@type": schema_type,
        "name": title or f"{business_type} ì›¹ì‚¬ì´íŠ¸",
        "description": description or f"{business_type} ì „ë¬¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
    }

    # í˜ì´ì§€ íƒ€ì…ë³„ ì¶”ê°€ ì†ì„±
    if page_type == "main":
        basic_schema["@type"] = ["Organization", schema_type]
    elif page_type == "contact":
        basic_schema["contactPoint"] = {
            "@type": "ContactPoint",
            "contactType": "customer service",
        }

    schema_html = f'<script type="application/ld+json">\n{json.dumps(basic_schema, ensure_ascii=False, indent=2)}\n</script>'

    return {
        "business_type": business_type,
        "page_type": page_type,
        "generated_schemas": [basic_schema],
        "schema_types": [schema_type],
        "html_schemas": [schema_html],
        "combined_html": schema_html,
        "extraction_notes": f"{business_type} ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©",
        "generation_method": "fallback_template",
        "success": True,
    }


@tool
def generate_jsonld_with_llm(
    business_type: str,
    seo_analysis: dict,
    page_type: str,
    api_key: str,
    url: str = "",
    use_llm_enhancement: bool = True,
) -> dict:
    """
    í•˜ì´ë¸Œë¦¬ë“œ JSON-LD ìƒì„±: í…œí”Œë¦¿ ìš°ì„  + LLM ë³´ì™„

    Args:
        business_type: ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…
        seo_analysis: SEO ë¶„ì„ ê²°ê³¼
        page_type: í˜ì´ì§€ íƒ€ì…
        api_key: OpenAI API í‚¤
        url: í˜ì´ì§€ URL
        use_llm_enhancement: Trueë©´ LLMìœ¼ë¡œ ê°œì„  ì‹œë„, Falseë©´ í…œí”Œë¦¿ë§Œ ì‚¬ìš©
    Returns:
        ìƒì„±ëœ JSON-LD ìŠ¤í‚¤ë§ˆë“¤
    """
    try:
        # print(f"      ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ JSON-LD ìƒì„±: {business_type} {page_type}")

        # 1ë‹¨ê³„: ì´ë¯¸ ë¶„ì„ëœ ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ (HTML ì¬íŒŒì‹± ì—†ìŒ)
        page_data = extract_page_data_from_analysis(seo_analysis, url, page_type)
        # print(f"         ğŸ“Š ê¸°ì¡´ ë¶„ì„ì—ì„œ ì¶”ì¶œ: {page_data['company_name']}")

        # 2ë‹¨ê³„: í…œí”Œë¦¿ ê¸°ë°˜ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ìƒì„±
        template_result = generate_template_schema(business_type, page_type, page_data)
        print(f"         âœ… í…œí”Œë¦¿ ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ")

        # 3ë‹¨ê³„: LLM ê°œì„  ì‹œë„ (ì„ íƒì ) - í•„ìš”ì‹œì—ë§Œ
        if api_key and use_llm_enhancement:
            # LLMì€ ì›ë³¸ HTMLì´ í•„ìš”í•˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
            # í•˜ì§€ë§Œ ì‹¤íŒ¨í•´ë„ í…œí”Œë¦¿ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥
            pass  # ì—¬ê¸°ì„œëŠ” í…œí”Œë¦¿ ìš°ì„ ìœ¼ë¡œ ë‹¨ìˆœí™”

        return template_result

    except Exception as e:
        print(f"      âŒ í•˜ì´ë¸Œë¦¬ë“œ JSON-LD ìƒì„± ì‹¤íŒ¨: {e}")
        return generate_fallback_jsonld(
            business_type,
            page_type,
            page_data.get("company_name", ""),
            page_data.get("description", ""),
        )


@tool
def generate_meta_tags_with_llm(
    html_content: str,
    url: str,
    page_type: str,
    seo_analysis: dict,
    business_type: str,
    api_key: str,
) -> dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ëœ ë©”íƒ€íƒœê·¸ ìƒì„±

    Args:
        html_content: HTML ì½˜í…ì¸ 
        url: í˜ì´ì§€ URL
        page_type: í˜ì´ì§€ íƒ€ì…
        seo_analysis: SEO ë¶„ì„ ê²°ê³¼
        business_type: ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…
        api_key: OpenAI API í‚¤

    Returns:
        ìƒì„±ëœ ë©”íƒ€íƒœê·¸ë“¤
    """
    try:
        # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if "<html" in html_content or "<div" in html_content:
            soup = BeautifulSoup(html_content, "html.parser")
            content_text = soup.get_text()
        else:
            content_text = html_content

        # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ
        if len(content_text) > 3000:
            content_text = content_text[:3000] + "..."

        if not api_key:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”íƒ€íƒœê·¸ ìƒì„±
            return generate_basic_meta_tags(seo_analysis, url, page_type, business_type)

        llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)

        meta_generation_prompt = f"""
        ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ SEO ìµœì í™”ëœ ë©”íƒ€íƒœê·¸ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        **í˜ì´ì§€ ì •ë³´:**
        - URL: {url}
        - í˜ì´ì§€ íƒ€ì…: {page_type}
        - ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…: {business_type}
        - í˜„ì¬ Title: {seo_analysis.get('title', {}).get('content', '')}
        - í˜„ì¬ Description: {seo_analysis.get('meta_description', {}).get('content', '')}
        
        **ì›¹í˜ì´ì§€ ì½˜í…ì¸ :**
        {content_text}
        
        **ìƒì„± ì§€ì¹¨:**
        1. Title: 30-60ì, í‚¤ì›Œë“œ í¬í•¨, í´ë¦­ ìœ ë„
        2. Meta Description: 120-160ì, ìš”ì•½ + í–‰ë™ ìœ ë„
        3. Keywords: ê´€ë ¨ í‚¤ì›Œë“œ 5-10ê°œ
        4. Open Graph íƒœê·¸: ì†Œì…œ ë¯¸ë””ì–´ ìµœì í™”
        5. Twitter Card íƒœê·¸
        6. ê¸°íƒ€ SEO ë©”íƒ€íƒœê·¸ë“¤
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        ```json
        {{
            "title": "ìµœì í™”ëœ íƒ€ì´í‹€",
            "meta_description": "ìµœì í™”ëœ ë©”íƒ€ ë””ìŠ¤í¬ë¦½ì…˜",
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
            "og_title": "Open Graph íƒ€ì´í‹€",
            "og_description": "Open Graph ë””ìŠ¤í¬ë¦½ì…˜",
            "og_type": "website",
            "twitter_title": "íŠ¸ìœ„í„° íƒ€ì´í‹€",
            "twitter_description": "íŠ¸ìœ„í„° ë””ìŠ¤í¬ë¦½ì…˜",
            "canonical_url": "{url}",
            "robots": "index, follow",
            "html_meta_tags": "ì™„ì„±ëœ HTML ë©”íƒ€íƒœê·¸ë“¤"
        }}
        ```
        """

        response = llm.invoke([HumanMessage(content=meta_generation_prompt)])
        response_text = response.content.strip()

        # JSON ë¸”ë¡ ì¶”ì¶œ
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()

        meta_data = json.loads(response_text)

        # HTML ë©”íƒ€íƒœê·¸ ìƒì„±
        html_tags = f"""
<title>{meta_data.get('title', '')}</title>
<meta name="description" content="{meta_data.get('meta_description', '')}">
<meta name="keywords" content="{', '.join(meta_data.get('keywords', []))}">
<meta name="robots" content="{meta_data.get('robots', 'index, follow')}">
<link rel="canonical" href="{meta_data.get('canonical_url', url)}">

<!-- Open Graph -->
<meta property="og:title" content="{meta_data.get('og_title', '')}">
<meta property="og:description" content="{meta_data.get('og_description', '')}">
<meta property="og:type" content="{meta_data.get('og_type', 'website')}">
<meta property="og:url" content="{url}">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="{meta_data.get('twitter_title', '')}">
<meta name="twitter:description" content="{meta_data.get('twitter_description', '')}">
"""

        return {
            "title": meta_data.get("title", ""),
            "meta_description": meta_data.get("meta_description", ""),
            "keywords": meta_data.get("keywords", []),
            "og_tags": {
                "title": meta_data.get("og_title", ""),
                "description": meta_data.get("og_description", ""),
                "type": meta_data.get("og_type", "website"),
            },
            "twitter_tags": {
                "title": meta_data.get("twitter_title", ""),
                "description": meta_data.get("twitter_description", ""),
            },
            "html_meta_tags": html_tags,
            "generation_method": "llm_optimized",
            "success": True,
        }

    except Exception as e:
        print(f"âš ï¸ ë©”íƒ€íƒœê·¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return generate_basic_meta_tags(seo_analysis, url, page_type, business_type)


def generate_basic_meta_tags(
    seo_analysis: dict, url: str, page_type: str, business_type: str
) -> dict:
    """ê¸°ë³¸ ë©”íƒ€íƒœê·¸ ìƒì„± (API í‚¤ ì—†ì„ ë•Œ)"""
    try:
        title = seo_analysis.get("title", {}).get("content", f"{page_type} í˜ì´ì§€")
        description = seo_analysis.get("meta_description", {}).get(
            "content", f"{business_type} {page_type} í˜ì´ì§€ì…ë‹ˆë‹¤."
        )

        # ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
        keywords = [business_type, page_type, "ì›¹ì‚¬ì´íŠ¸"]

        html_tags = f"""
<title>{title}</title>
<meta name="description" content="{description}">
<meta name="keywords" content="{', '.join(keywords)}">
<meta name="robots" content="index, follow">
<link rel="canonical" href="{url}">

<!-- Open Graph -->
<meta property="og:title" content="{title}">
<meta property="og:description" content="{description}">
<meta property="og:type" content="website">
<meta property="og:url" content="{url}">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="{title}">
<meta name="twitter:description" content="{description}">
"""

        return {
            "title": title,
            "meta_description": description,
            "keywords": keywords,
            "og_tags": {"title": title, "description": description, "type": "website"},
            "twitter_tags": {"title": title, "description": description},
            "html_meta_tags": html_tags,
            "generation_method": "basic_template",
            "success": True,
        }
    except Exception as e:
        return {
            "title": "",
            "meta_description": "",
            "keywords": [],
            "html_meta_tags": "",
            "generation_method": "fallback",
            "success": False,
            "error": str(e),
        }


@tool
def generate_intelligent_faq_with_llm(
    website_content: str, business_type: str, api_key: str, keywords: list
) -> dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì§€ëŠ¥í˜• FAQ ìƒì„±

    Args:
        website_content: ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ 
        business_type: ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…
        api_key: OpenAI API í‚¤
        keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ìƒì„±ëœ FAQ ë°ì´í„°
    """
    try:
        if not api_key:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ FAQ ìƒì„±
            return generate_basic_faq(business_type)

        if "<html" in website_content or "<div" in website_content:
            soup = BeautifulSoup(website_content, "html.parser")
            content_text = soup.get_text()
        else:
            content_text = website_content

        # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ
        if len(content_text) > 3000:
            content_text = content_text[:3000] + "..."

        llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)

        faq_prompt = f"""
        ë‹¤ìŒ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ {business_type}ì— ì í•©í•œ FAQë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        **ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…**: {business_type}
        **ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ **: {content_text}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ 5-7ê°œì˜ FAQë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        {{
            "generated_faqs": [
                {{
                    "question": "ê³ ê°ì´ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸",
                    "answer": "ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€",
                    "category": "ì„œë¹„ìŠ¤/ê°€ê²©/ì ˆì°¨ ë“±"
                }}
            ],
            "faq_count": ìƒì„±ëœ_FAQ_ê°œìˆ˜,
            "business_context": "ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ ì„¤ëª…"
        }}
        
        JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """

        response = llm.invoke([HumanMessage(content=faq_prompt)])
        response_text = response.content.strip()

        # JSON íŒŒì‹±
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()

        faq_data = json.loads(response_text)

        return {
            "generated_faqs": faq_data.get("generated_faqs", []),
            "faq_count": len(faq_data.get("generated_faqs", [])),
            "business_context": faq_data.get("business_context", ""),
            "generation_method": "llm_intelligent",
            "success": True,
        }

    except Exception as e:
        print(f"âš ï¸ FAQ ìƒì„± ì‹¤íŒ¨: {e}")
        return generate_basic_faq(business_type)


def generate_basic_faq(business_type: str) -> dict:
    """ê¸°ë³¸ FAQ ìƒì„± (API í‚¤ ì—†ì„ ë•Œ)"""
    basic_faqs = [
        {
            "question": f"{business_type} ì„œë¹„ìŠ¤ëŠ” ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?",
            "answer": f"ë‹¤ì–‘í•œ {business_type} ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.",
            "category": "ì„œë¹„ìŠ¤",
        },
        {
            "question": "ìƒë‹´ì€ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
            "answer": "ì „í™”ë‚˜ ì˜¨ë¼ì¸ìœ¼ë¡œ ìƒë‹´ ì˜ˆì•½ì„ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "category": "ìƒë‹´",
        },
        {
            "question": "ìš´ì˜ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "answer": "í‰ì¼ ì˜¤ì „ 9ì‹œë¶€í„° ì˜¤í›„ 6ì‹œê¹Œì§€ ìš´ì˜í•©ë‹ˆë‹¤.",
            "category": "ìš´ì˜",
        },
    ]

    return {
        "generated_faqs": basic_faqs,
        "faq_count": len(basic_faqs),
        "business_context": f"{business_type} ê¸°ë³¸ FAQ",
        "generation_method": "basic_template",
        "success": True,
    }


# ===== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ =====
# user_input_nodeë¥¼ db_input_nodeë¡œ ë³€ê²½
def db_input_node(state: MultiPageSEOWorkflowState) -> MultiPageSEOWorkflowState:
    """DBì—ì„œ ë¸Œëœë“œ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ë…¸ë“œ"""
    print("ğŸš€ DB ê¸°ë°˜ SEO/GEO ìµœì í™” ì‹œìŠ¤í…œ")
    print("=" * 60)

    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœì‹  ë¸Œëœë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    try:
        db_manager = DatabaseManager()
        brand_info = db_manager.get_latest_brand_url()
        db_manager.close()
    except Exception as e:
        print(f"âš ï¸ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
        brand_info = {
            # "id": 1,
            "brand_official_name": "uniformbridge",
            "official_site_url": "https://uniformbridge.com/",  # ë˜ëŠ” ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ URL
            "updated_at": datetime.now(),
        }
        if not brand_info:
            state["messages"] = [
                AIMessage(
                    content="âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¸Œëœë“œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            ]
            state["next_action"] = "handle_error"
            return state

    # ë¸Œëœë“œ ì •ë³´ ì„¤ì •
    state["brand_info"] = brand_info
    state["user_url"] = brand_info["official_site_url"]
    state["business_type"] = brand_info["brand_official_name"]

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        state["user_mode"] = "analyze"
    else:
        state["user_mode"] = "full"
        print("âœ… OpenAI API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    state["api_key"] = api_key or ""
    state["max_pages"] = 9
    state["target_keywords"] = ""
    state["crawl_results"] = []
    state["crawled_files"] = []
    state["page_analyses"] = []

    # ì‹œì‘ ë©”ì‹œì§€ ì¶”ê°€
    state["messages"] = [
        SystemMessage(
            content="ë‹¹ì‹ ì€ DB ê¸°ë°˜ ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        ),
        HumanMessage(
            content=f"ë¸Œëœë“œ {brand_info['brand_official_name']}ì˜ ì›¹ì‚¬ì´íŠ¸ {state['user_url']}ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
        ),
    ]

    state["current_stage"] = "db_input_received"
    state["next_action"] = "start_crawling"

    print(f"âœ… DBì—ì„œ ë¸Œëœë“œ ì •ë³´ ë¡œë“œ ì™„ë£Œ:")
    print(f"   â€¢ ë¸Œëœë“œ: {brand_info['brand_official_name']}")
    print(f"   â€¢ URL: {state['user_url']}")
    print("ğŸ“Š ê³§ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    return state


def crawling_node(state: MultiPageSEOWorkflowState) -> MultiPageSEOWorkflowState:
    """ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ë…¸ë“œ"""
    print("ğŸ•·ï¸ ì‚¬ì´íŠ¸ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘...")

    try:
        url = state["user_url"]
        max_pages = state["max_pages"]

        # ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰
        # print(f"   ğŸ“Š ìµœëŒ€ {max_pages}ê°œ í˜ì´ì§€ í¬ë¡¤ë§...")
        crawl_result = crawl_full_website.invoke(
            {"base_url": url, "max_pages": max_pages}
        )

        if crawl_result.get("error"):
            state["messages"].append(
                AIMessage(content=f"í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_result['error']}")
            )
            state["next_action"] = "handle_error"
            return state

        crawl_results = crawl_result["crawl_results"]
        state["crawl_results"] = crawl_results

        # í¬ë¡¤ë§ëœ íŒŒì¼ ëª©ë¡ ì €ì¥
        crawled_files = [
            result["filename"] for result in crawl_results if "filename" in result
        ]
        state["crawled_files"] = crawled_files

        # print(f"   âœ… {len(crawl_results)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ")

        # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬ ì¶œë ¥
        page_types = crawl_result["page_types"]
        # print("   ğŸ“‹ í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬:")
        for page_type, count in page_types.items():
            pass
            # print(f"      â€¢ {page_type.upper()}: {count}ê°œ")

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        state["messages"].append(
            AIMessage(
                content=f"""
        ğŸ•·ï¸ ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤:
        
        ğŸ“Š **í¬ë¡¤ë§ ê²°ê³¼**:
        â€¢ ì´ í˜ì´ì§€ ìˆ˜: {len(crawl_results)}ê°œ
        â€¢ í˜ì´ì§€ íƒ€ì…: {', '.join([f'{k}({v})' for k, v in page_types.items()])}
        â€¢ ìƒì„±ëœ íŒŒì¼: {len(crawled_files)}ê°œ
        
        ğŸ“ **ìƒì„±ëœ HTML íŒŒì¼ë“¤**:
        {chr(10).join(['â€¢ ' + f for f in crawled_files[:10]])}
        {'â€¢ ...' if len(crawled_files) > 10 else ''}
        
        ë‹¤ìŒ ë‹¨ê³„ë¡œ ê° í˜ì´ì§€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.
        """
            )
        )

        state["current_stage"] = "crawling_completed"
        state["next_action"] = "start_page_analysis"

    except Exception as e:
        state["messages"].append(AIMessage(content=f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"))
        state["next_action"] = "handle_error"
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    return state


def enhanced_page_analysis_node(
    state: MultiPageSEOWorkflowState,
) -> MultiPageSEOWorkflowState:
    """ê°•í™”ëœ í˜ì´ì§€ ë¶„ì„ ë…¸ë“œ - JSON-LD ìë™ ìƒì„± í¬í•¨"""
    print("ğŸ“Š ê° í˜ì´ì§€ë³„ SEO/GEO + JSON-LD ë¶„ì„ ì¤‘...")

    try:
        crawl_results = state["crawl_results"]
        page_analyses = []

        # ê° í˜ì´ì§€ë³„ í†µí•© ë¶„ì„ + JSON-LD ìƒì„±
        for i, result in enumerate(crawl_results, 1):
            url = result["url"]
            page_type = result["page_type"]
            html_content = result.get("html_content", "")

            # print(f"   ğŸ“„ [{page_type.upper()}] ë¶„ì„ ì¤‘... ({i}/{len(crawl_results)})")

            # ê¸°ì¡´ SEO ë¶„ì„
            seo_analysis = analyze_page_from_html.invoke(
                {"html_content": html_content, "url": url, "page_type": page_type}
            )

            # GEO ë¶„ì„
            geo_analysis = calculate_site_geo_scores.invoke({"crawl_results": [result]})

            if not seo_analysis.get("error"):
                # ğŸ†• JSON-LD ìë™ ìƒì„± ì¶”ê°€
                print(f"      ğŸ¤– JSON-LD ìë™ ìƒì„±...")
                jsonld_result = generate_jsonld_with_llm.invoke(
                    {
                        "business_type": state.get("business_type", "ì¼ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤"),
                        "seo_analysis": seo_analysis,
                        "page_type": page_type,
                        "api_key": state["api_key"],
                        "url": url,
                    }
                )

                # í†µí•© ë¶„ì„ ê²°ê³¼ì— JSON-LD ì •ë³´ ì¶”ê°€
                enhanced_analysis = {
                    "url": url,
                    "page_type": page_type,
                    "seo_score": seo_analysis.get("seo_score", 0),
                    "geo_score": geo_analysis.get("site_geo_score", 0),
                    "seo_analysis": seo_analysis,
                    "geo_analysis": geo_analysis,
                    "jsonld_schemas": jsonld_result,  # ğŸ†• JSON-LD ì •ë³´ ì¶”ê°€
                    "title": seo_analysis.get("title", {}),
                    "meta_description": seo_analysis.get("meta_description", {}),
                    "headings": seo_analysis.get("headings", {}),
                    "images": seo_analysis.get("images", {}),
                    "links": seo_analysis.get("links", {}),
                    "structured_data": seo_analysis.get("structured_data", {}),
                    "content_quality": seo_analysis.get("content_quality", {}),
                }

                page_analyses.append(enhanced_analysis)
                print(
                    f"      âœ… SEO: {seo_analysis.get('seo_score', 0)}/100, JSON-LD: {len(jsonld_result.get('generated_schemas', []))}ê°œ ìŠ¤í‚¤ë§ˆ"
                )
            else:
                print(f"      âŒ ë¶„ì„ ì‹¤íŒ¨")

        state["page_analyses"] = page_analyses

        # ì‚¬ì´íŠ¸ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        site_seo_result = calculate_site_average_scores.invoke(
            {"page_analyses": page_analyses}
        )
        state["site_seo_analysis"] = site_seo_result

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ì— JSON-LD ì •ë³´ ì¶”ê°€
        total_schemas = sum(
            len(p.get("jsonld_schemas", {}).get("generated_schemas", []))
            for p in page_analyses
        )

        state["messages"].append(
            AIMessage(
                content=f"""
        ğŸ“Š ê°œë³„ í˜ì´ì§€ SEO/GEO + JSON-LD ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤:
        
        **ë¶„ì„ëœ í˜ì´ì§€**: {len(page_analyses)}ê°œ
        **ìƒì„±ëœ JSON-LD ìŠ¤í‚¤ë§ˆ**: {total_schemas}ê°œ
        
        **í˜ì´ì§€ë³„ JSON-LD ìƒì„± í˜„í™©**:
        {chr(10).join([f'â€¢ [{p["page_type"].upper()}] {len(p.get("jsonld_schemas", {}).get("generated_schemas", []))}ê°œ ìŠ¤í‚¤ë§ˆ' for p in page_analyses])}
        
        ë‹¤ìŒ ë‹¨ê³„ë¡œ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
        """
            )
        )

        state["current_stage"] = "enhanced_analysis_completed"
        state["next_action"] = "site_geo_analysis"

    except Exception as e:
        state["messages"].append(
            AIMessage(content=f"ê°•í™”ëœ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        )
        state["next_action"] = "handle_error"
        print(f"âŒ ê°•í™”ëœ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")

    return state


def site_geo_analysis_node(
    state: MultiPageSEOWorkflowState,
) -> MultiPageSEOWorkflowState:
    """ì‚¬ì´íŠ¸ ì „ì²´ GEO ë¶„ì„ ë…¸ë“œ"""
    print("ğŸ¯ ì‚¬ì´íŠ¸ ì „ì²´ GEO ë¶„ì„ ì¤‘...")

    try:
        crawl_results = state["crawl_results"]

        # ì‚¬ì´íŠ¸ ì „ì²´ GEO ì ìˆ˜ ê³„ì‚°
        print("   ğŸ“Š 6ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì „ì²´ GEO ì ìˆ˜ ê³„ì‚°...")
        geo_result = calculate_site_geo_scores.invoke({"crawl_results": crawl_results})

        if geo_result.get("error"):
            print(f"   âŒ GEO ë¶„ì„ ì‹¤íŒ¨: {geo_result['error']}")
            state["site_geo_analysis"] = {"error": geo_result["error"]}
        else:
            state["site_geo_analysis"] = geo_result

            print(f"   âœ… ì‚¬ì´íŠ¸ ì „ì²´ GEO ì ìˆ˜: {geo_result['site_geo_score']:.1f}/100")
            print(f"   ğŸ“‹ ì„¸ë¶€ ì ìˆ˜:")
            site_averages = geo_result["site_averages"]
            for criterion, score in site_averages.items():
                print(f"      â€¢ {criterion}: {score:.1f}/100")

        state["current_stage"] = "geo_analysis_completed"
        state["next_action"] = "generate_meta"

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        if not geo_result.get("error"):
            state["messages"].append(
                AIMessage(
                    content=f"""
            ğŸ¯ ì‚¬ì´íŠ¸ ì „ì²´ GEO ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤:
            
            **ì „ì²´ ì‚¬ì´íŠ¸ GEO ì ìˆ˜**: {geo_result['site_geo_score']:.1f}/100
            **ë¶„ì„ëœ í˜ì´ì§€**: {geo_result['pages_analyzed']}ê°œ
            
            **ì„¸ë¶€ ì ìˆ˜ (ì‚¬ì´íŠ¸ í‰ê· )**:
            â€¢ ëª…í™•ì„± (Clarity): {site_averages.get('clarity', 0):.1f}/100
            â€¢ êµ¬ì¡°ì„± (Structure): {site_averages.get('structure', 0):.1f}/100
            â€¢ ë§¥ë½ì„± (Context): {site_averages.get('context', 0):.1f}/100
            â€¢ ì •í•©ì„± (Alignment): {site_averages.get('alignment', 0):.1f}/100
            â€¢ ì‹œì˜ì„± (Timeliness): {site_averages.get('timeliness', 0):.1f}/100
            â€¢ ë…ì°½ì„± (Originality): {site_averages.get('originality', 0):.1f}/100
            
            **ì‚¬ì´íŠ¸ ì „ì²´ ì¶”ì²œì‚¬í•­**:
            {chr(10).join(['â€¢ ' + rec for rec in geo_result.get('recommendations', [])])}
            """
                )
            )

        print("âœ… ì‚¬ì´íŠ¸ ì „ì²´ GEO ë¶„ì„ ì™„ë£Œ")

    except Exception as e:
        state["messages"].append(AIMessage(content=f"ì‚¬ì´íŠ¸ GEO ë¶„ì„ ì‹¤íŒ¨: {str(e)}"))
        state["next_action"] = "handle_error"
        print(f"âŒ ì‚¬ì´íŠ¸ GEO ë¶„ì„ ì‹¤íŒ¨: {e}")

    return state


def meta_tags_generation_node(
    state: MultiPageSEOWorkflowState,
) -> MultiPageSEOWorkflowState:
    """ë©”íƒ€íƒœê·¸ ìë™ ìƒì„± ì „ìš© ë…¸ë“œ"""
    print("ğŸ·ï¸ ë©”íƒ€íƒœê·¸ ìë™ ìƒì„± ì¤‘...")

    try:
        crawl_results = state["crawl_results"]
        page_analyses = state["page_analyses"]
        business_type = state["business_type"]
        api_key = state["api_key"]
        meta_results = {}

        for result in crawl_results:
            url = result["url"]
            page_type = result["page_type"]
            html_content = result.get("html_content", "")

            page_analysis = next((p for p in page_analyses if p["url"] == url), {})

            # print(f"   ğŸ·ï¸ [{page_type.upper()}] ë©”íƒ€íƒœê·¸ ìƒì„± ì¤‘...")

            meta_result = generate_meta_tags_with_llm.invoke(
                {
                    "html_content": html_content,
                    "url": url,
                    "page_type": page_type,
                    "seo_analysis": page_analysis.get("seo_analysis", {}),
                    "business_type": business_type,
                    "api_key": api_key,
                }
            )

            meta_results[url] = meta_result
            print(f"      âœ… ë©”íƒ€íƒœê·¸ ìƒì„± ì™„ë£Œ")

        state["meta_results"] = meta_results
        state["current_stage"] = "meta_generated"
        state["next_action"] = "generate_faqs"

        # print(f"âœ… ë©”íƒ€íƒœê·¸ ìƒì„± ì™„ë£Œ: {len(meta_results)}ê°œ í˜ì´ì§€")

        return state

    except Exception as e:
        print(f"âŒ ë©”íƒ€íƒœê·¸ ìƒì„± ì‹¤íŒ¨: {e}")
        state["next_action"] = "handle_error"
        return state


def faq_generation_node(state: MultiPageSEOWorkflowState) -> MultiPageSEOWorkflowState:
    """ì§€ëŠ¥í˜• FAQ ìƒì„± ì „ìš© ë…¸ë“œ"""
    print("ğŸ“‹ ì§€ëŠ¥í˜• FAQ ìƒì„± ì¤‘...")

    try:
        crawl_results = state["crawl_results"]
        business_type = state["business_type"]
        api_key = state["api_key"]
        faq_results = {}

        for result in crawl_results:
            url = result["url"]
            page_type = result["page_type"]
            html_content = result.get("html_content", "")

            # print(f"   ğŸ“‹ [{page_type.upper()}] FAQ ìƒì„± ì¤‘...")

            faq_result = generate_intelligent_faq_with_llm.invoke(
                {
                    "website_content": html_content,
                    "business_type": business_type,
                    "api_key": api_key,
                    "keywords": [],
                }
            )

            faq_results[url] = faq_result
            # print(f"      âœ… {len(faq_result.get('generated_faqs', []))}ê°œ FAQ ìƒì„±")

        state["faq_results"] = faq_results
        state["current_stage"] = "faq_generated"
        state["next_action"] = "final_optimization"

        total_faqs = sum(len(r.get("generated_faqs", [])) for r in faq_results.values())
        print(f"âœ… FAQ ìƒì„± ì™„ë£Œ")
        # print(f"âœ… FAQ ìƒì„± ì™„ë£Œ: ì´ {total_faqs}ê°œ FAQ")

        return state

    except Exception as e:
        print(f"âŒ FAQ ìƒì„± ì‹¤íŒ¨: {e}")
        state["next_action"] = "handle_error"
        return state


def final_optimization_node(
    state: MultiPageSEOWorkflowState,
) -> MultiPageSEOWorkflowState:
    """ìµœì¢… HTML ìƒì„± ë…¸ë“œ - ëª¨ë“  ìš”ì†Œ í†µí•©"""
    print("ğŸ“„ ìµœì¢… HTML íŒŒì¼ ìƒì„± ì¤‘...")

    try:
        crawl_results = state["crawl_results"]
        meta_results = state.get("meta_results", {})
        faq_results = state.get("faq_results", {})

        final_html_files = []

        for result in crawl_results:
            url = result["url"]
            page_type = result["page_type"]
            original_html = result.get("html_content", "")

            # print(f"   ğŸ“„ [{page_type.upper()}] ìµœì¢… HTML ìƒì„± ì¤‘...")

            # ê° ìš”ì†Œ ë°ì´í„° ìˆ˜ì§‘
            meta_data = meta_results.get(url, {})
            faq_data = faq_results.get(url, {})

            # JSON-LD ë°ì´í„°ëŠ” page_analysesì—ì„œ ê°€ì ¸ì˜¤ê¸°
            page_analysis = next(
                (p for p in state.get("page_analyses", []) if p["url"] == url), {}
            )
            jsonld_data = page_analysis.get("jsonld_schemas", {})

            try:
                # HTML í†µí•© ìƒì„±
                final_html, h1_optimization = integrate_all_elements_to_html(
                    original_html, meta_data, jsonld_data, faq_data, url, page_type
                )

                # íŒŒì¼ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"final_complete_{page_type}_{timestamp}.html"
                filepath = f"outputs/{filename}"

                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(final_html)

                final_html_files.append(
                    {
                        "url": url,
                        "page_type": page_type,
                        "filename": filepath,  # outputs/ ê²½ë¡œ í¬í•¨
                        "has_meta": bool(meta_data.get("success")),
                        "has_jsonld": bool(jsonld_data.get("success")),
                        "has_faq": bool(faq_data.get("success")),
                        "h1_optimization": h1_optimization,
                    }
                )

                print(f"      âœ… ì €ì¥ ì™„ë£Œ: {filename}")

            except Exception as file_error:
                print(f"      âŒ {page_type} íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {file_error}")
                continue

        state["final_html_files"] = final_html_files
        state["output_files"] = state.get("output_files", [])
        state["output_files"].extend([f["filename"] for f in final_html_files])
        state["current_stage"] = "html_generated"
        state["next_action"] = "generate_summary"

        print(f"âœ… ìµœì¢… HTML ìƒì„± ì™„ë£Œ")
        # print(f"âœ… ìµœì¢… HTML ìƒì„± ì™„ë£Œ: {len(final_html_files)}ê°œ íŒŒì¼")

        return state

    except Exception as e:
        import traceback

        print(f"âŒ ìµœì¢… HTML ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"ğŸ“ ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜:")
        print(traceback.format_exc())

        state["final_html_files"] = []
        state["next_action"] = "handle_error"
        return state


def integrate_all_elements_to_html(
    original_html: str,
    meta_data: dict,
    jsonld_data: dict,
    faq_data: dict,
    url: str,
    page_type: str,
) -> tuple:
    """ê°„ë‹¨í•œ HTML í†µí•©"""
    # print(f"ğŸ”§ HTML í†µí•© ì‹œì‘: {page_type} í˜ì´ì§€")
    print(f"ğŸ”§ HTML í†µí•© ì‹œì‘")

    try:
        soup = BeautifulSoup(original_html, "html.parser")

        # 1. ë©”íƒ€íƒœê·¸ ì ìš©
        if meta_data.get("success"):
            title_tag = soup.find("title")
            if title_tag:
                title_tag.string = meta_data.get("title", "")
            else:
                new_title = soup.new_tag("title")
                new_title.string = meta_data.get("title", "")
                if soup.head:
                    soup.head.append(new_title)

        # 2. JSON-LD ìŠ¤í‚¤ë§ˆ ì ìš©
        if jsonld_data.get("success"):
            html_schemas = jsonld_data.get("html_schemas", [])
            for schema_html in html_schemas[:2]:  # ìµœëŒ€ 2ê°œë§Œ
                if schema_html and soup.head:
                    schema_soup = BeautifulSoup(schema_html, "html.parser")
                    script_tag = schema_soup.find("script")
                    if script_tag:
                        soup.head.append(script_tag)

        # 3. H1 íƒœê·¸ ìµœì í™”
        h1_optimization = optimize_h1_tags(soup, page_type)

        # 4. FAQ ì¶”ê°€ (FAQ í˜ì´ì§€ì—ë§Œ)
        if page_type == "faq" and faq_data.get("success"):
            faqs = faq_data.get("generated_faqs", [])
            if faqs and soup.body:
                faq_section = soup.new_tag("section", id="auto-generated-faq")
                faq_h2 = soup.new_tag("h2")
                faq_h2.string = "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸"
                faq_section.append(faq_h2)

                for faq in faqs[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                    faq_div = soup.new_tag("div", class_="faq-item")

                    q_h3 = soup.new_tag("h3")
                    q_h3.string = faq.get("question", "")
                    faq_div.append(q_h3)

                    a_p = soup.new_tag("p")
                    a_p.string = faq.get("answer", "")
                    faq_div.append(a_p)

                    faq_section.append(faq_div)

                soup.body.append(faq_section)

        return str(soup), h1_optimization

    except Exception as e:
        print(f"   âŒ HTML í†µí•© ì‹¤íŒ¨: {e}")
        return original_html, {"action_taken": "failed", "error": str(e)}


def optimize_h1_tags(soup, page_type: str) -> dict:
    """H1 íƒœê·¸ ìµœì í™”"""
    h1_tags = soup.find_all("h1")
    h1_count = len(h1_tags)

    result = {
        "original_h1_count": h1_count,
        "action_taken": "none",
        "final_h1_count": h1_count,
    }

    if h1_count == 0:
        # H1 íƒœê·¸ê°€ ì—†ìœ¼ë©´ ìˆ¨ê²¨ì§„ H1 ìƒì„±
        hidden_h1 = soup.new_tag("h1")
        hidden_h1["style"] = (
            "position:absolute;left:-10000px;top:auto;width:1px;height:1px;overflow:hidden;"
        )

        # í˜ì´ì§€ íƒ€ì…ì— ë”°ë¥¸ ì ì ˆí•œ ì œëª© ìƒì„±
        if page_type == "main":
            hidden_h1.string = "ë©”ì¸ í˜ì´ì§€"
        elif page_type == "about":
            hidden_h1.string = "íšŒì‚¬ì†Œê°œ"
        elif page_type == "service":
            hidden_h1.string = "ì„œë¹„ìŠ¤ ì†Œê°œ"
        elif page_type == "contact":
            hidden_h1.string = "ì—°ë½ì²˜"
        elif page_type == "product":
            hidden_h1.string = "ìƒí’ˆ ì •ë³´"
        elif page_type == "faq":
            hidden_h1.string = "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸"
        else:
            hidden_h1.string = f"{page_type.title()} í˜ì´ì§€"

        if soup.body:
            soup.body.insert(0, hidden_h1)

        result.update(
            {
                "action_taken": "created_hidden_h1",
                "final_h1_count": 1,
                "h1_content": hidden_h1.string,
            }
        )

    elif h1_count > 1:
        # ì—¬ëŸ¬ ê°œì˜ H1ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” H2ë¡œ ë³€ê²½
        first_h1 = h1_tags[0]

        for i, h1 in enumerate(h1_tags[1:], 1):
            new_h2 = soup.new_tag("h2")

            # ëª¨ë“  ì†ì„± ë³µì‚¬
            for attr, value in h1.attrs.items():
                new_h2[attr] = value

            # ë‚´ìš© ë³µì‚¬
            new_h2.clear()
            for content in h1.contents:
                new_h2.append(content)

            h1.replace_with(new_h2)

        result.update(
            {
                "action_taken": "converted_multiple_h1_to_h2",
                "final_h1_count": 1,
                "h1_content": first_h1.get_text(),
                "original_h1_converted": h1_count - 1,
            }
        )

    else:
        # H1ì´ ì •í™•íˆ 1ê°œë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
        result.update(
            {
                "action_taken": "maintained_single_h1",
                "final_h1_count": 1,
                "h1_content": h1_tags[0].get_text(),
            }
        )

    return result


def multi_page_summary_node(
    state: MultiPageSEOWorkflowState,
) -> MultiPageSEOWorkflowState:
    """ë‹¤ì¤‘ í˜ì´ì§€ ì¢…í•© ìš”ì•½ ë…¸ë“œ"""
    print("ğŸ“‹ ì‚¬ì´íŠ¸ ì „ì²´ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")

    try:
        if not state.get("api_key"):
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ìš”ì•½ë§Œ ìƒì„±
            basic_summary = generate_basic_summary(state)
            state["final_summary"] = basic_summary
            state["current_stage"] = "completed"
            state["next_action"] = "end"
            return state

        llm = ChatOpenAI(model="gpt-4o-mini", api_key=state["api_key"])

        # ì¢…í•© ë¶„ì„ ë©”íƒ€ë°ì´í„° ìƒì„±
        summary_metadata = generate_multi_page_summary_metadata(state)

        # ì½˜ì†” ì¶œë ¥
        print_multi_page_analysis_summary(state, summary_metadata)

        summary_prompt = generate_multi_page_summary_prompt(state, summary_metadata)

        summary_messages = state["messages"] + [HumanMessage(content=summary_prompt)]
        summary_response = llm.invoke(summary_messages)

        # ìµœì¢… ìš”ì•½ ë°ì´í„° êµ¬ì„±
        final_summary = create_multi_page_final_summary(
            state, summary_metadata, summary_response.content
        )
        state["final_summary"] = final_summary

        # ì¢…í•© ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„±
        report_files = generate_multi_page_report_files(
            state, summary_metadata, summary_response.content
        )
        state["output_files"].extend(report_files)

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        state["messages"].append(summary_response)

        state["current_stage"] = "completed"
        state["next_action"] = "end"

        # ìµœì¢… ì™„ë£Œ ìš”ì•½ ì¶œë ¥
        print_multi_page_final_summary(state, summary_metadata)

    except Exception as e:
        state["messages"].append(AIMessage(content=f"ì¢…í•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"))
        # API í‚¤ ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ìš”ì•½ ìƒì„±
        basic_summary = generate_basic_summary(state)
        state["final_summary"] = basic_summary
        state["next_action"] = "end"
        print(f"âŒ ì¢…í•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©: {e}")

    return state


def generate_basic_summary(state: MultiPageSEOWorkflowState) -> dict:
    """API í‚¤ ì—†ì„ ë•Œ ê¸°ë³¸ ìš”ì•½ ìƒì„±"""
    crawl_results = state.get("crawl_results", [])
    page_analyses = state.get("page_analyses", [])
    site_seo = state.get("site_seo_analysis", {})
    site_geo = state.get("site_geo_analysis", {})

    return {
        "success": True,
        "workflow_type": "LangGraph Multi-Page SEO/GEO Optimization",
        "website": state["user_url"],
        "business_type": state["business_type"],
        "processing_mode": state["user_mode"],
        "crawling_results": {
            "pages_crawled": len(crawl_results),
            "pages_analyzed": len(page_analyses),
        },
        "scores": {
            "site_seo": site_seo.get("site_average_seo_score", 0),
            "site_geo": (
                site_geo.get("site_geo_score", 0) if not site_geo.get("error") else 0
            ),
        },
        "output_files": state.get("output_files", []),
        "completion_time": datetime.now().isoformat(),
        "note": "API í‚¤ ì—†ì´ ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ",
    }


def error_handler_node(state: MultiPageSEOWorkflowState) -> MultiPageSEOWorkflowState:
    """ì—ëŸ¬ ì²˜ë¦¬ ë…¸ë“œ"""
    print("âŒ ì—ëŸ¬ ì²˜ë¦¬ ì¤‘...")

    # ì—ëŸ¬ ë¡œê·¸ ìƒì„±
    error_messages = [msg for msg in state["messages"] if "ì‹¤íŒ¨" in str(msg.content)]

    error_summary = {
        "success": False,
        "errors": [str(msg.content) for msg in error_messages],
        "current_stage": state.get("current_stage", "unknown"),
        "url": state.get("user_url", ""),
        "max_pages": state.get("max_pages", 0),
        "pages_crawled": len(state.get("crawl_results", [])),
        "pages_analyzed": len(state.get("page_analyses", [])),
        "timestamp": datetime.now().isoformat(),
    }

    state["final_summary"] = error_summary
    state["current_stage"] = "error"
    state["next_action"] = "end"

    # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    error_file = f"outputs/db_seo_error_{timestamp}.json"

    with open(error_file, "w", encoding="utf-8") as f:
        json.dump(error_summary, f, ensure_ascii=False, indent=2)

    state["output_files"] = [error_file]

    print(f"ğŸ“ ì—ëŸ¬ ë¡œê·¸ ì €ì¥: {error_file}")
    for error in error_summary["errors"]:
        print(f"   â€¢ {error}")

    return state


# ===== ë‹¤ì¤‘ í˜ì´ì§€ ìš”ì•½ ë° ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜ë“¤ =====
def generate_multi_page_summary_metadata(state: MultiPageSEOWorkflowState) -> dict:
    """ë‹¤ì¤‘ í˜ì´ì§€ ë¶„ì„ ë©”íƒ€ë°ì´í„° ìƒì„±"""

    crawl_results = state.get("crawl_results", [])
    page_analyses = state.get("page_analyses", [])
    site_seo = state.get("site_seo_analysis", {})
    site_geo = state.get("site_geo_analysis", {})
    final_html_files = state.get("final_html_files", [])
    h1_analysis = analyze_h1_optimizations(final_html_files)

    metadata = {
        "crawl_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "workflow_type": "LangGraph ë‹¤ì¤‘í˜ì´ì§€ SEO/GEO ìµœì í™”",
        "base_url": state.get("user_url", ""),
        "max_pages_requested": state.get("max_pages", 0),
        "pages_crawled": len(crawl_results),
        "pages_analyzed": len(page_analyses),
        "pages_optimized": len(final_html_files),
        "business_type": state.get("business_type", "Unknown"),
        "processing_mode": state.get("user_mode", "unknown"),
        "h1_optimization_results": h1_analysis,
        # ì ìˆ˜ ë° ë“±ê¸‰
        "site_seo_score": site_seo.get("site_average_seo_score", 0),
        "site_geo_score": (
            site_geo.get("site_geo_score", 0) if not site_geo.get("error") else 0
        ),
        "overall_grade": "C+",
        # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬
        "page_type_distribution": {},
        "page_type_averages": site_seo.get("page_type_averages", {}),
        # GEO ì„¸ë¶€ ì ìˆ˜
        "geo_details": (
            site_geo.get("site_averages", {}) if not site_geo.get("error") else {}
        ),
        # ë¬¸ì œì  ë° ê°œì„ ì‚¬í•­
        "common_issues": site_seo.get("common_issues", {}),
        "site_geo_recommendations": (
            site_geo.get("recommendations", []) if not site_geo.get("error") else []
        ),
        "priority_improvements": site_seo.get("priority_improvements", []),
        # íŒŒì¼ ìƒì„±
        "files_generated": len(state.get("output_files", [])),
        # ROI ë° ì˜ˆìƒ íš¨ê³¼
        "investment_required": "ê°œë°œì 40-80ì‹œê°„ (ì•½ 200-400ë§Œì›)",
        "expected_roi": "4-8ê°œì›” ë‚´ 400-800% ROI",
        "timeline_to_results": "ìµœì í™” í›„ 2-6ê°œì›”",
    }

    # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬ ê³„ì‚°
    page_types = {}
    for result in crawl_results:
        page_type = result.get("page_type", "other")
        page_types[page_type] = page_types.get(page_type, 0) + 1
    metadata["page_type_distribution"] = page_types

    # ì „ì²´ ì ìˆ˜ ê¸°ë°˜ ë“±ê¸‰ ê³„ì‚°
    avg_score = (metadata["site_seo_score"] + metadata["site_geo_score"]) / 2
    if avg_score >= 90:
        metadata["overall_grade"] = "A+"
    elif avg_score >= 80:
        metadata["overall_grade"] = "A"
    elif avg_score >= 70:
        metadata["overall_grade"] = "B+"
    elif avg_score >= 60:
        metadata["overall_grade"] = "B"
    else:
        metadata["overall_grade"] = "C+"

    return metadata


def analyze_h1_optimizations(final_html_files: list) -> dict:
    """H1 íƒœê·¸ ìµœì í™” ê²°ê³¼ ë¶„ì„"""
    total_pages = len(final_html_files)

    if total_pages == 0:
        return {}

    original_no_h1 = 0
    original_multiple_h1 = 0
    original_single_h1 = 0
    actions_taken = []

    for file_info in final_html_files:
        h1_opt = file_info.get("h1_optimization", {})
        original_count = h1_opt.get("original_h1_count", 0)
        action = h1_opt.get("action_taken", "none")

        if original_count == 0:
            original_no_h1 += 1
        elif original_count == 1:
            original_single_h1 += 1
        else:
            original_multiple_h1 += 1

        actions_taken.append(
            {
                "page_type": file_info.get("page_type", "unknown"),
                "original_h1_count": original_count,
                "action": action,
                "final_h1_content": h1_opt.get("h1_content", ""),
            }
        )

    return {
        "total_pages_analyzed": total_pages,
        "original_no_h1_pages": original_no_h1,
        "original_single_h1_pages": original_single_h1,
        "original_multiple_h1_pages": original_multiple_h1,
        "pages_needed_h1_creation": original_no_h1,
        "pages_needed_h1_consolidation": original_multiple_h1,
        "h1_optimization_actions": actions_taken,
        "h1_compliance_rate": (
            round((original_single_h1 / total_pages) * 100, 1) if total_pages > 0 else 0
        ),
    }


def print_multi_page_analysis_summary(state: MultiPageSEOWorkflowState, metadata: dict):
    """ë‹¤ì¤‘ í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ ì½˜ì†” ì¶œë ¥"""

    print(f"\n{'='*70}")
    # print("ğŸš€ ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ë¶„ì„ ì™„ë£Œ")
    print("ğŸš€ SEO/GEO ë¶„ì„ ì™„ë£Œ")
    print(f"{'='*70}")

    # í¬ë¡¤ë§ ì •ë³´
    print(f"ğŸ•·ï¸ í¬ë¡¤ë§ ì •ë³´:")
    print(f"   - ê¸°ë³¸ URL: {metadata['base_url']}")
    # print(f"   - ìš”ì²­ í˜ì´ì§€: ìµœëŒ€ {metadata['max_pages_requested']}ê°œ")
    # print(f"   - í¬ë¡¤ë§ëœ í˜ì´ì§€: {metadata['pages_crawled']}ê°œ")
    # print(f"   - ë¶„ì„ëœ í˜ì´ì§€: {metadata['pages_analyzed']}ê°œ")
    # print(f"   - ìµœì í™”ëœ í˜ì´ì§€: {metadata['pages_optimized']}ê°œ")
    print(f"   - ë¶„ì„ ì‹œê°„: {metadata['crawl_timestamp']}")

    # H1 íƒœê·¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ì¶”ê°€
    h1_analysis = metadata.get("h1_optimization_results", {})
    if h1_analysis:
        print(f"\nğŸ·ï¸ H1 íƒœê·¸ ìµœì í™” ê²°ê³¼:")
        print(f"   - ë¶„ì„ëœ í˜ì´ì§€: {h1_analysis.get('total_pages_analyzed', 0)}ê°œ")
        print(
            f"   - ì›ë˜ H1 ì—†ë˜ í˜ì´ì§€: {h1_analysis.get('original_no_h1_pages', 0)}ê°œ â†’ ìˆ¨ê²¨ì§„ H1 ìƒì„±"
        )
        print(
            f"   - ì›ë˜ H1 ì—¬ëŸ¬ê°œ í˜ì´ì§€: {h1_analysis.get('original_multiple_h1_pages', 0)}ê°œ â†’ H2ë¡œ ë³€í™˜"
        )
        print(
            f"   - ì›ë˜ H1 ì ì ˆí•œ í˜ì´ì§€: {h1_analysis.get('original_single_h1_pages', 0)}ê°œ â†’ ìœ ì§€"
        )
        print(
            f"   - ìµœì¢… H1 ì¤€ìˆ˜ìœ¨: {h1_analysis.get('h1_compliance_rate', 0)}% â†’ 100%"
        )

    # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬
    print(f"\nğŸ“Š í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬:")
    for page_type, count in metadata["page_type_distribution"].items():
        avg_score = metadata["page_type_averages"].get(page_type, 0)
        print(f"   â€¢ {page_type.upper()}: {count}ê°œ (í‰ê·  {avg_score}/100)")

    # ì¢…í•© í‰ê°€
    print(f"\nğŸ“‹ ì‚¬ì´íŠ¸ ì¢…í•© í‰ê°€:")
    print(f"   ì „ì²´ ë“±ê¸‰: {metadata['overall_grade']}")
    print(f"   ì‚¬ì´íŠ¸ SEO ì ìˆ˜: {metadata['site_seo_score']:.1f}/100")
    print(f"   ì‚¬ì´íŠ¸ GEO ì ìˆ˜: {metadata['site_geo_score']:.1f}/100")

    # GEO ì„¸ë¶€ ì ìˆ˜
    if metadata["geo_details"]:
        print(f"\nğŸ¯ GEO ì„¸ë¶€ ì ìˆ˜ (ì‚¬ì´íŠ¸ í‰ê· ):")
        for criterion, score in metadata["geo_details"].items():
            print(f"   â€¢ {criterion}: {score:.1f}/100")

    # ì£¼ìš” ë¬¸ì œì 
    if metadata["common_issues"]:
        print(f"\nâš ï¸ ì‚¬ì´íŠ¸ ê³µí†µ ë¬¸ì œì :")
        for issue_type, percentage in metadata["common_issues"].items():
            if percentage >= 50:
                issue_name = {
                    "title_issues": "Title íƒœê·¸ ë¬¸ì œ",
                    "description_issues": "Meta Description ë¬¸ì œ",
                    "h1_issues": "H1 íƒœê·¸ ë¬¸ì œ",
                    "alt_issues": "ALT ì†ì„± ë¬¸ì œ",
                    "schema_issues": "êµ¬ì¡°í™”ëœ ë°ì´í„° ëˆ„ë½",
                    "content_issues": "ì½˜í…ì¸  í’ˆì§ˆ ë¬¸ì œ",
                }.get(issue_type, issue_type)
                print(f"   â€¢ {issue_name}: {percentage}% í˜ì´ì§€ ì˜í–¥")

    # ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­
    if metadata["priority_improvements"]:
        print(f"\nğŸš¨ ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­:")
        for i, improvement in enumerate(metadata["priority_improvements"][:5], 1):
            print(f"   {i}. {improvement.get('issue', '')}")
            print(f"      â†’ ì˜í–¥ ë²”ìœ„: {improvement.get('affected_pages', '')} í˜ì´ì§€")
            print(f"      â†’ ì˜ˆìƒ íš¨ê³¼: {improvement.get('impact', '')}")

    # ROI ì˜ˆì¸¡
    print(f"\nğŸ’° ROI ì˜ˆì¸¡ (ì‚¬ì´íŠ¸ ì „ì²´):")
    print(f"   - í•„ìš” íˆ¬ì: {metadata['investment_required']}")
    print(f"   - ì˜ˆìƒ ìˆ˜ìµë¥ : {metadata['expected_roi']}")
    print(f"   - ê²°ê³¼ê¹Œì§€ ê¸°ê°„: {metadata['timeline_to_results']}")


def generate_multi_page_summary_prompt(
    state: MultiPageSEOWorkflowState, metadata: dict
) -> str:
    """ë‹¤ì¤‘ í˜ì´ì§€ ì¢…í•© ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    h1_info = ""
    h1_analysis = metadata.get("h1_optimization_results", {})
    if h1_analysis:
        h1_info = f"""
        **H1 íƒœê·¸ ìµœì í™” ê²°ê³¼:**
        - H1 ì—†ë˜ í˜ì´ì§€: {h1_analysis.get('original_no_h1_pages', 0)}ê°œ (ìˆ¨ê²¨ì§„ H1 ìë™ ìƒì„±)
        - H1 ì—¬ëŸ¬ê°œ í˜ì´ì§€: {h1_analysis.get('original_multiple_h1_pages', 0)}ê°œ (H2ë¡œ ë³€í™˜)
        - H1 ì¤€ìˆ˜ìœ¨: {h1_analysis.get('h1_compliance_rate', 0)}% â†’ 100% ë‹¬ì„±
        """

    geo_detailed_prompt = (
        f"""
    **GEO ì„¸ë¶€ ë¶„ì„ (AI ê²€ìƒ‰ì—”ì§„ ìµœì í™”):**
    
    **í˜„ì¬ GEO ì ìˆ˜ ë¶„ì„:**
    - Clarity (ëª…í™•ì„±): {metadata['geo_details'].get('clarity', 0):.1f}/100
    - Structure (êµ¬ì¡°ì„±): {metadata['geo_details'].get('structure', 0):.1f}/100  
    - Context (ë§¥ë½ì„±): {metadata['geo_details'].get('context', 0):.1f}/100
    - Alignment (ì •í•©ì„±): {metadata['geo_details'].get('alignment', 0):.1f}/100
    - Timeliness (ì‹œì˜ì„±): {metadata['geo_details'].get('timeliness', 0):.1f}/100
    - Originality (ë…ì°½ì„±): {metadata['geo_details'].get('originality', 0):.1f}/100
    """
        if metadata["geo_details"]
        else ""
    )

    prompt = f"""
    ë‹¤ì¤‘ í˜ì´ì§€ LangGraph SEO/GEO ìµœì í™” ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
    ì‚¬ì´íŠ¸ ì „ì²´ë¥¼ í¬ë¡¤ë§í•˜ê³  ë¶„ì„í•œ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    **ì‚¬ì´íŠ¸ ì •ë³´:**
    - ê¸°ë³¸ URL: {state['user_url']}
    - ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…: {state['business_type']}
    - ì²˜ë¦¬ ëª¨ë“œ: {state['user_mode']}
    
    **í¬ë¡¤ë§ ê²°ê³¼:**
    - ìš”ì²­ í˜ì´ì§€: ìµœëŒ€ {metadata['max_pages_requested']}ê°œ
    - í¬ë¡¤ë§ëœ í˜ì´ì§€: {metadata['pages_crawled']}ê°œ
    - ë¶„ì„ëœ í˜ì´ì§€: {metadata['pages_analyzed']}ê°œ
    - í˜ì´ì§€ íƒ€ì… ë¶„í¬: {metadata['page_type_distribution']}
    
    **ì¢…í•© ë¶„ì„ ê²°ê³¼:**
    - ì „ì²´ ë“±ê¸‰: {metadata['overall_grade']}
    - ì‚¬ì´íŠ¸ SEO ì ìˆ˜: {metadata['site_seo_score']}/100
    - ì‚¬ì´íŠ¸ GEO ì ìˆ˜: {metadata['site_geo_score']}/100
    
    {h1_info}

    **í˜ì´ì§€ íƒ€ì…ë³„ í‰ê·  ì ìˆ˜:**
    {chr(10).join([f'- {k.upper()}: {v}/100' for k, v in metadata['page_type_averages'].items()])}
    
    {geo_detailed_prompt}
    
    **ì‚¬ì´íŠ¸ ê³µí†µ ë¬¸ì œì :**
    {chr(10).join([f'- {k}: {v}% í˜ì´ì§€ ì˜í–¥' for k, v in metadata['common_issues'].items() if v >= 50])}
    
    **ROI ì •ë³´:**
    - í•„ìš” íˆ¬ì: {metadata['investment_required']}
    - ì˜ˆìƒ ìˆ˜ìµë¥ : {metadata['expected_roi']}
    - ê²°ê³¼ê¹Œì§€ ê¸°ê°„: {metadata['timeline_to_results']}
    
    ë‹¤ìŒ êµ¬ì¡°ë¡œ ìƒì„¸í•˜ê³  ì‹¤ìš©ì ì¸ ë‹¤ì¤‘ í˜ì´ì§€ ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
    
    # ğŸš€ ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ìµœì í™” ì™„ë£Œ ë¦¬í¬íŠ¸
    
    ## ğŸ“Š ì‹¤í–‰ ìš”ì•½ (Executive Summary)
    - í•µì‹¬ ë©”ì‹œì§€ ë° ì „ì²´ ì„±ê³¼
    - ì‚¬ì´íŠ¸ ì „ì²´ ì ìˆ˜ ë° ë“±ê¸‰
    - í¬ë¡¤ë§ ë° ìµœì í™” ê·œëª¨
    
    ## ğŸ•·ï¸ í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„
    ### ğŸ“„ í˜ì´ì§€ ìˆ˜ì§‘ í˜„í™©
    ### ğŸ¯ í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬ ë° ì ìˆ˜
    ### ğŸ“Š ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„
    
    ## ğŸ” ì‚¬ì´íŠ¸ ì „ì²´ SEO/GEO ë¶„ì„
    ### ğŸ“ˆ ì¢…í•© ì ìˆ˜ í˜„í™©
    ### ğŸ¯ GEO 6ê°€ì§€ ê¸°ì¤€ ì‚¬ì´íŠ¸ í‰ê·  (AI ê²€ìƒ‰ì—”ì§„ ìµœì í™”)
    
    ## ğŸ› ï¸ ì ìš©ëœ ë‹¤ì¤‘ í˜ì´ì§€ ìµœì í™”
    ### âœ… í˜ì´ì§€ë³„ ìµœì í™” ê²°ê³¼
    ### ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤
    ### ğŸ”§ ì‚¬ì´íŠ¸ ì „ì²´ ê°œì„ ì‚¬í•­
    
    ## ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼ ë° ROI (ì‚¬ì´íŠ¸ ì „ì²´)
    ### ë‹¨ê¸° íš¨ê³¼ (1-8ì£¼)
    ### ì¤‘ê¸° íš¨ê³¼ (2-6ê°œì›”)
    ### ì¥ê¸° íš¨ê³¼ (6ê°œì›”-1ë…„)
    ### ğŸ’° ì‚¬ì´íŠ¸ ì „ì²´ íˆ¬ì ëŒ€ë¹„ ìˆ˜ìµ ì „ë§
    
    ## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­
    ### ì¦‰ì‹œ ì‹¤í–‰ í•­ëª© (ìš°ì„ ìˆœìœ„ë³„)
    ### ì¤‘ê¸° ì‚¬ì´íŠ¸ ê°œì„  ê³„íš
    ### ì¥ê¸° SEO/GEO ì „ëµ
    
    ## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì§€ì† ê´€ë¦¬ ë°©ì•ˆ
    ### ì¶”ì í•´ì•¼ í•  ì§€í‘œ (í˜ì´ì§€ë³„/ì‚¬ì´íŠ¸ ì „ì²´)
    ### ê¶Œì¥ ë„êµ¬ ë° ì£¼ê¸°
    ### ì½˜í…ì¸  ì—…ë°ì´íŠ¸ ì „ëµ

    ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ê³¼ ì‚¬ì´íŠ¸ ì „ì²´ ìµœì í™”ì˜ ê°€ì¹˜ë¥¼ ê°•ì¡°í•˜ê³ ,
    ê¸°ì¡´ ë‹¨ì¼ í˜ì´ì§€ ë¶„ì„ ëŒ€ë¹„ ì¥ì ì„ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”.
    """

    return prompt


def create_multi_page_final_summary(
    state: MultiPageSEOWorkflowState, metadata: dict, report_content: str
) -> dict:
    """ë‹¤ì¤‘ í˜ì´ì§€ ìµœì¢… ìš”ì•½ ë°ì´í„° ìƒì„±"""

    return {
        "success": True,
        "workflow_type": "LangGraph Multi-Page SEO/GEO Optimization",
        "website": state["user_url"],
        "business_type": state["business_type"],
        "processing_mode": state["user_mode"],
        # í¬ë¡¤ë§ ê²°ê³¼
        "crawling_results": {
            "max_pages_requested": metadata["max_pages_requested"],
            "pages_crawled": metadata["pages_crawled"],
            "pages_analyzed": metadata["pages_analyzed"],
            "pages_optimized": metadata["pages_optimized"],
            "page_type_distribution": metadata["page_type_distribution"],
        },
        # ì ìˆ˜ ë° ë“±ê¸‰
        "scores": {
            "overall_grade": metadata["overall_grade"],
            "site_seo": metadata["site_seo_score"],
            "site_geo": metadata["site_geo_score"],
            "page_type_averages": metadata["page_type_averages"],
        },
        # GEO ì„¸ë¶€ ì ìˆ˜
        "geo_details": metadata["geo_details"],
        # ë¶„ì„ ê²°ê³¼
        "analysis_results": {
            "common_issues": metadata["common_issues"],
            "priority_improvements": metadata["priority_improvements"],
            "site_geo_recommendations": metadata["site_geo_recommendations"],
        },
        # ROI ì •ë³´
        "roi_projection": {
            "investment_required": metadata["investment_required"],
            "expected_roi": metadata["expected_roi"],
            "timeline_to_results": metadata["timeline_to_results"],
        },
        # íŒŒì¼ ë° ì¶œë ¥
        "output_files": state.get("output_files", []),
        "files_generated": metadata["files_generated"],
        # ë¦¬í¬íŠ¸ ë‚´ìš©
        "detailed_report": report_content,
        "completion_time": datetime.now().isoformat(),
        # ë‹¤ìŒ ë‹¨ê³„
        "next_steps": {
            "immediate": [
                imp.get("issue", "") for imp in metadata["priority_improvements"][:3]
            ],
            "monitoring_required": True,
            "follow_up_recommended": "4-8ì£¼ í›„ ì¬ë¶„ì„ (ì‚¬ì´íŠ¸ ì „ì²´)",
        },
    }


def generate_multi_page_report_files(
    state: MultiPageSEOWorkflowState, metadata: dict, report_content: str
) -> list:
    """ë‹¤ì¤‘ í˜ì´ì§€ ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ìƒì„±"""

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    generated_files = []

    # 1. ë©”ì¸ ë¦¬í¬íŠ¸ (ë§ˆí¬ë‹¤ìš´)
    main_report_file = f"outputs/db_seo_geo_report_{timestamp}.md"

    with open(main_report_file, "w", encoding="utf-8") as f:
        f.write(f"# ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ìµœì í™” ì™„ë£Œ ë¦¬í¬íŠ¸\n\n")
        f.write(f"**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**ì›Œí¬í”Œë¡œìš°**: {metadata['workflow_type']}\n")
        f.write(f"**ëŒ€ìƒ ì›¹ì‚¬ì´íŠ¸**: {state['user_url']}\n")
        f.write(f"**í¬ë¡¤ë§ ê·œëª¨**: {metadata['pages_crawled']}ê°œ í˜ì´ì§€\n")
        f.write(f"**ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ì…**: {state['business_type']}\n\n")
        f.write("---\n\n")
        f.write(report_content)

        # ì¶”ê°€ ë‹¤ì¤‘ í˜ì´ì§€ ë©”íƒ€ë°ì´í„°
        f.write(f"\n\n## ğŸ”§ ë‹¤ì¤‘ í˜ì´ì§€ ë¶„ì„ ë©”íƒ€ë°ì´í„°\n\n")
        f.write(f"### í¬ë¡¤ë§ ì •ë³´\n")
        f.write(f"- **ìš”ì²­ í˜ì´ì§€ ìˆ˜**: {metadata['max_pages_requested']}ê°œ\n")
        f.write(f"- **ì‹¤ì œ í¬ë¡¤ë§ëœ í˜ì´ì§€**: {metadata['pages_crawled']}ê°œ\n")
        f.write(f"- **ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ í˜ì´ì§€**: {metadata['pages_analyzed']}ê°œ\n")
        f.write(f"- **ìµœì í™” ëŒ€ìƒ í˜ì´ì§€**: {metadata['pages_optimized']}ê°œ\n\n")

        f.write(f"### í˜ì´ì§€ íƒ€ì…ë³„ ìƒì„¸ ì •ë³´\n")
        for page_type, count in metadata["page_type_distribution"].items():
            avg_score = metadata["page_type_averages"].get(page_type, 0)
            f.write(
                f"- **{page_type.upper()}**: {count}ê°œ í˜ì´ì§€, í‰ê·  SEO ì ìˆ˜ {avg_score}/100\n"
            )

        if metadata["geo_details"]:
            f.write(f"\n### ì‚¬ì´íŠ¸ ì „ì²´ GEO ì ìˆ˜ ìƒì„¸\n")
            for criterion, score in metadata["geo_details"].items():
                f.write(f"- **{criterion}**: {score:.1f}/100\n")

    generated_files.append(main_report_file)

    # 2. í˜ì´ì§€ë³„ ìƒì„¸ ë°ì´í„° (JSON)
    detailed_data_file = f"outputs/db_detailed_data_{timestamp}.json"

    detailed_data = {
        "metadata": metadata,
        "crawling_results": state.get("crawl_results", []),
        "page_analyses": state.get("page_analyses", []),
        "site_analysis": {
            "seo": state.get("site_seo_analysis", {}),
            "geo": state.get("site_geo_analysis", {}),
        },
        "generation_timestamp": datetime.now().isoformat(),
    }

    with open(detailed_data_file, "w", encoding="utf-8") as f:
        json.dump(detailed_data, f, ensure_ascii=False, indent=2)

    generated_files.append(detailed_data_file)

    return generated_files


def print_multi_page_final_summary(state: MultiPageSEOWorkflowState, metadata: dict):
    """ë‹¤ì¤‘ í˜ì´ì§€ ìµœì¢… ì™„ë£Œ ìš”ì•½ ì¶œë ¥"""

    print(f"\n{'='*70}")
    print("ğŸ‰ SEO/GEO ìµœì í™” ì™„ë£Œ!")
    # print("ğŸ‰ ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ìµœì í™” ì™„ë£Œ!")
    print(f"{'='*70}")

    # ì „ì²´ ê·œëª¨
    # print(f"ğŸ“Š ì²˜ë¦¬ ê·œëª¨:")
    # print(f"   â€¢ í¬ë¡¤ë§ëœ í˜ì´ì§€: {metadata['pages_crawled']}ê°œ")
    # print(f"   â€¢ ë¶„ì„ëœ í˜ì´ì§€: {metadata['pages_analyzed']}ê°œ")
    # print(f"   â€¢ ìµœì í™”ëœ í˜ì´ì§€: {metadata['pages_optimized']}ê°œ")
    # print(f"   â€¢ ìƒì„±ëœ íŒŒì¼: {metadata['files_generated']}ê°œ")

    # ìµœì¢… ì ìˆ˜
    print(f"\nğŸ“Š ì‚¬ì´íŠ¸ ì „ì²´ ìµœì¢… ì ìˆ˜:")
    print(f"   ì „ì²´ ë“±ê¸‰: {metadata['overall_grade']}")
    print(f"   ì‚¬ì´íŠ¸ SEO: {metadata['site_seo_score']:.1f}/100")
    print(f"   ì‚¬ì´íŠ¸ GEO: {metadata['site_geo_score']:.1f}/100")

    # í˜ì´ì§€ íƒ€ì…ë³„ ì„±ê³¼
    print(f"\nğŸ“„ í˜ì´ì§€ íƒ€ì…ë³„ ì„±ê³¼:")
    for page_type, avg_score in metadata["page_type_averages"].items():
        count = metadata["page_type_distribution"].get(page_type, 0)
        print(f"í‰ê·  {avg_score}/100")
        # print(f"   â€¢ {page_type.upper()}: {count}ê°œ í˜ì´ì§€, í‰ê·  {avg_score}/100")

    # ìƒì„±ëœ íŒŒì¼ë“¤
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for file in state["output_files"]:
        print(f"   â€¢ {file}")

    # ROI ìš”ì•½
    print(f"\nğŸ’° ì‚¬ì´íŠ¸ ì „ì²´ ROI ìš”ì•½:")
    print(f"   í•„ìš” íˆ¬ì: {metadata['investment_required']}")
    print(f"   ì˜ˆìƒ ìˆ˜ìµë¥ : {metadata['expected_roi']}")
    print(f"   íš¨ê³¼ ë°œí˜„ ê¸°ê°„: {metadata['timeline_to_results']}")

    print(f"\nâœ¨ LangGraph SEO/GEO ìµœì í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸŒ ì‚¬ì´íŠ¸ ì „ì²´ê°€ ê²€ìƒ‰ì—”ì§„ê³¼ AI ê²€ìƒ‰ì—”ì§„ì— ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")


# ===== ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜ =====
def route_next_action(state: MultiPageSEOWorkflowState) -> str:
    """ë‹¤ìŒ ì•¡ì…˜ìœ¼ë¡œ ë¼ìš°íŒ…"""
    return state.get("next_action", "end")


# ===== ì›Œí¬í”Œë¡œìš° ìƒì„± =====
def create_optimized_seo_langgraph() -> StateGraph:
    """3ë‹¨ê³„ ë¶„ë¦¬í˜• ìµœì í™” ì›Œí¬í”Œë¡œìš°"""

    workflow = StateGraph(MultiPageSEOWorkflowState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("db_input", db_input_node)
    workflow.add_node("crawling", crawling_node)
    workflow.add_node("page_analysis", enhanced_page_analysis_node)
    workflow.add_node("site_geo_analysis", site_geo_analysis_node)
    workflow.add_node("meta_tags_generation", meta_tags_generation_node)
    workflow.add_node("faq_generation", faq_generation_node)
    workflow.add_node("final_optimization", final_optimization_node)
    workflow.add_node("multi_page_summary", multi_page_summary_node)
    workflow.add_node("error_handler", error_handler_node)

    # ì—£ì§€ ì—°ê²°
    workflow.add_conditional_edges(
        "db_input",
        route_next_action,
        {"start_crawling": "crawling", "handle_error": "error_handler", "end": END},
    )

    workflow.add_conditional_edges(
        "crawling",
        route_next_action,
        {
            "start_page_analysis": "page_analysis",
            "handle_error": "error_handler",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "page_analysis",
        route_next_action,
        {
            "site_geo_analysis": "site_geo_analysis",
            "generate_summary": "multi_page_summary",
            "handle_error": "error_handler",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "site_geo_analysis",
        route_next_action,
        {
            "generate_meta": "meta_tags_generation",
            "generate_summary": "multi_page_summary",
            "handle_error": "error_handler",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "meta_tags_generation",
        route_next_action,
        {
            "generate_faqs": "faq_generation",
            "handle_error": "error_handler",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "faq_generation",
        route_next_action,
        {
            "final_optimization": "final_optimization",
            "handle_error": "error_handler",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "final_optimization",
        route_next_action,
        {
            "generate_summary": "multi_page_summary",
            "handle_error": "error_handler",
            "end": END,
        },
    )

    workflow.add_conditional_edges(
        "multi_page_summary",
        route_next_action,
        {"end": END, "handle_error": "error_handler"},
    )

    workflow.add_edge("error_handler", END)
    workflow.set_entry_point("db_input")

    return workflow


# ===== ë©”ì¸ ì‹¤í–‰ í´ë˜ìŠ¤ =====
class CompleteMultiPageSEOOptimizer:
    """ì™„ì „ í†µí•© ë‹¤ì¤‘ í˜ì´ì§€ SEO/GEO ìµœì í™”ê¸°"""

    def __init__(self):
        self.workflow = create_optimized_seo_langgraph()
        memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=memory)

    async def optimize_full_website(
        self,
        url: str = None,
        api_key: str = None,
        mode: str = "full",
        max_pages: int = 9,
    ) -> Dict:
        """ë‹¤ì¤‘ í˜ì´ì§€ ì›¹ì‚¬ì´íŠ¸ ìµœì í™” ì‹¤í–‰"""

        initial_state = {
            "user_url": url or "",
            "api_key": api_key or "",
            "user_mode": mode,
            "max_pages": max_pages,
            "messages": [],
            "current_stage": "starting",
            "next_action": "start_crawling",
            "crawl_results": [],
            "crawled_files": [],
            "page_analyses": [],
            "site_seo_analysis": {},
            "site_geo_analysis": {},
            "site_performance": {},
            "site_structured_data": {},
            "site_keywords": {},
            "optimized_pages": [],
            "optimization_summary": {},
            "meta_results": {},
            "jsonld_results": {},
            "faq_results": {},
            "final_optimization": [],
            "final_html_files": [],
            "business_type": "",
            "target_keywords": "",
            "output_files": [],
            "final_summary": {},
        }

        config = {"configurable": {"thread_id": "multipage_seo_geo_session"}}

        try:
            final_state = await self.app.ainvoke(initial_state, config)
            return final_state["final_summary"]
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }


# ===== ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í•¨ìˆ˜ë“¤ =====
async def quick_multi_page_optimize(
    url: str, api_key: str = None, mode: str = "full", max_pages: int = 9
) -> Dict:
    """ë¹ ë¥¸ ë‹¤ì¤‘ í˜ì´ì§€ ìµœì í™”"""
    optimizer = CompleteMultiPageSEOOptimizer()
    return await optimizer.optimize_full_website(url, api_key, mode, max_pages)


def interactive_multi_page_optimize():
    """ëŒ€í™”í˜• ë‹¤ì¤‘ í˜ì´ì§€ ìµœì í™”"""
    optimizer = CompleteMultiPageSEOOptimizer()

    print("ğŸš€ ì›¹í˜ì´ì§€ LangGraph SEO/GEO ìµœì í™” ì‹œìŠ¤í…œ")
    # print("ğŸš€ ì™„ì „ í†µí•© ë‹¤ì¤‘ í˜ì´ì§€ LangGraph SEO/GEO ìµœì í™” ì‹œìŠ¤í…œ")
    # print("=" * 70)
    # print("ğŸ•·ï¸ sitemap.xml ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í˜ì´ì§€ í¬ë¡¤ë§")
    print("ğŸ”¥ ì‚¬ì´íŠ¸ ì „ì²´ ë¶„ì„ + ìë™ ìµœì í™” + ì¢…í•© ë¦¬í¬íŠ¸")
    print("ğŸ“Š í˜ì´ì§€ë³„ ë¶„ì„ ë° ì‚¬ì´íŠ¸ ì „ì²´ í‰ê·  ì ìˆ˜ ì œê³µ")
    print()

    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰ìƒí™© í‘œì‹œ
    for chunk in optimizer.app.stream(
        {
            "user_url": "",
            "api_key": "",
            "user_mode": "full",
            "max_pages": 9,
            "messages": [],
            "current_stage": "starting",
            "next_action": "start_crawling",
            "crawl_results": [],
            "crawled_files": [],
            "page_analyses": [],
            "site_seo_analysis": {},
            "site_geo_analysis": {},
            "site_performance": {},
            "site_structured_data": {},
            "site_keywords": {},
            "optimized_pages": [],
            "optimization_summary": {},
            "meta_results": {},
            "jsonld_results": {},
            "faq_results": {},
            "final_optimization": [],
            "final_html_files": [],
            "business_type": "",
            "target_keywords": "",
            "output_files": [],
            "final_summary": {},
        },
        {"configurable": {"thread_id": "interactive_session"}},
    ):
        stage = list(chunk.keys())[0]
        print(f"ğŸ”„ í˜„ì¬ ë‹¨ê³„: {stage}")


# ===== ë©”ì¸ ì‹¤í–‰ =====
async def main():
    print("ğŸš€ DB ê¸°ë°˜ ì›¹í˜ì´ì§€ LangGraph SEO/GEO ìµœì í™” ì‹œìŠ¤í…œ")
    print("=" * 70)
    print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœì‹  ë¸Œëœë“œ URL ìë™ ê°€ì ¸ì˜¤ê¸°")
    print("ğŸ“ ê²°ê³¼ë¬¼ì€ outputs/ í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤")
    print()

    # .env íŒŒì¼ ì²´í¬
    if not os.path.exists(".env"):
        print("âš ï¸ .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return

    # ë°”ë¡œ ì‹¤í–‰ (ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ)
    try:
        optimizer = CompleteMultiPageSEOOptimizer()
        result = await optimizer.optimize_full_website()

        if result.get("success"):
            print("\nğŸ‰ DB ê¸°ë°˜ ì›¹í˜ì´ì§€ SEO/GEO ìµœì í™” ì™„ë£Œ!")
            # ... ê²°ê³¼ ì¶œë ¥ ...
        else:
            print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())
