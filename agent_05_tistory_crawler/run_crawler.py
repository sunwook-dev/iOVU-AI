#!/usr/bin/env python3
"""
Tistory Blog Crawler Runner
í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
import sys
import os
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawler import TistoryCrawler
from database.queries.brand_queries import BrandQueries
from config import Config



def crawl_single_brand(brand_official_name, crawler=None):
    """ë‹¨ì¼ ë¸Œëœë“œ í¬ë¡¤ë§ (brand_official_name ê¸°ë°˜)"""
    brand = BrandQueries.get_brand_by_name(brand_official_name)
    if not brand:
        print(f"âŒ ë¸Œëœë“œëª… '{brand_official_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ” ë¸Œëœë“œ ì •ë³´:")
    print(f"   - ì´ë¦„: {brand['brand_official_name']}")

    if crawler is None:
        crawler = TistoryCrawler()

    result = crawler.crawl_brand_blogs(
        brand_official_name=brand["brand_official_name"],
        max_pages=Config.MAX_PAGES_PER_BRAND,
    )
    return result


def crawl_all_brands():
    """ëª¨ë“  ë¸Œëœë“œ í¬ë¡¤ë§"""
    brands = BrandQueries.list_brands()
    if not brands:
        print("âŒ í¬ë¡¤ë§í•  ë¸Œëœë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“‹ ì´ {len(brands)}ê°œ ë¸Œëœë“œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
    print("-" * 60)

    crawler = TistoryCrawler()
    results = []
    try:
        for idx, brand in enumerate(brands, 1):
            print(f"\n[{idx}/{len(brands)}] ë¸Œëœë“œ: {brand['brand_official_name']}")
            try:
                result = crawl_single_brand(brand["brand_official_name"], crawler)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
    finally:
        crawler.close_driver()

    print("\n" + "=" * 60)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
    print("-" * 60)
    total_crawled = sum(r["crawled_count"] for r in results)
    total_saved = sum(r["saved_count"] for r in results)
    print(f"âœ… ì„±ê³µí•œ ë¸Œëœë“œ: {len(results)}ê°œ")
    print(f"ğŸ“„ í¬ë¡¤ë§í•œ í¬ìŠ¤íŠ¸: {total_crawled}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ëœ í¬ìŠ¤íŠ¸: {total_saved}ê°œ")

# main í•¨ìˆ˜ ì •ì˜
def main():
    parser = argparse.ArgumentParser(
        description="í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # íŠ¹ì • ë¸Œëœë“œ í¬ë¡¤ë§
  python run_crawler.py --brand-id 1
  
  # ëª¨ë“  ë¸Œëœë“œ í¬ë¡¤ë§
  python run_crawler.py --all
  
  # ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ í¬ë¡¤ë§
  python run_crawler.py --brand-id 1 --max-pages 5
        """,
    )

    # í¬ë¡¤ë§ ëŒ€ìƒ ì„ íƒ
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--brand-id", type=int, help="íŠ¹ì • ë¸Œëœë“œ IDë¡œ í¬ë¡¤ë§")
    group.add_argument("--brand-name", type=str, help="íŠ¹ì • ë¸Œëœë“œ ê³µì‹ëª…(brand_official_name)ìœ¼ë¡œ í¬ë¡¤ë§")
    group.add_argument("--all", action="store_true", help="ëª¨ë“  ë¸Œëœë“œ í¬ë¡¤ë§")

    # ì˜µì…˜
    parser.add_argument(
        "--max-pages",
        type=int,
        default=Config.MAX_PAGES_PER_BRAND,
        help=f"ë¸Œëœë“œë‹¹ ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: {Config.MAX_PAGES_PER_BRAND})",
    )

    args = parser.parse_args()

    # ì„¤ì • í™•ì¸
    Config.ensure_directories()

    print("ğŸš€ í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“· ìµœì†Œ ì´ë¯¸ì§€ ê°œìˆ˜: {Config.MIN_IMAGES}ê°œ")
    print("-" * 60)

    crawler = None

    try:
        if args.brand_id:
            print("âŒ í˜„ì¬ brand_id ê¸°ë°˜ í¬ë¡¤ë§ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. --brand-nameì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return
        elif args.brand_name:
            if args.max_pages:
                Config.MAX_PAGES_PER_BRAND = args.max_pages
            crawler = TistoryCrawler()
            crawl_single_brand(args.brand_name, crawler)
        else:
            crawl_all_brands()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        if crawler:
            crawler.close_driver()

    print(f"\nâ° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()
